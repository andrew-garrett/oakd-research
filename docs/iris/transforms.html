<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>iris.transforms API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>iris.transforms</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#################### IMPORTS ####################
#################################################


from typing import List, Optional, Sequence, Tuple

import torch
from torchvision import transforms as T
from torchvision.transforms import functional as F

from iris.autocapture import detect_eyes

#################### CUSTOM TRANSFORM PRESETS ####################
##################################################################


class PresetTrain(torch.nn.Module):
    &#34;&#34;&#34;
    Custom Composed Transforms for training

    Compatible with Segmentation and Classification Models

    Training augmentations:

        1. ConvertImageDtype(torch.float)
        2. RandomAutocaptureCrop()
        3. RandomResize(base_size, base_size)
        4. RandomPerspective()
        5. RandomCrop(int(crop_size * base_size))
        6. StrideResize()
        7. RandomHorizontalFlip(hflip_prob)
    &#34;&#34;&#34;

    def __init__(
        self,
        task: str = &#34;segmentation&#34;,
        base_size: int = 320,
        crop_size: float = 0.7,
        hflip_prob: float = 0.5,
        mean: Optional[List[float]] = None,
        std: Optional[List[float]] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - task: the learning task
            - base_size: the desired approximate size of resized images
            - crop_size: the desired size of cropped images as a percentage of base_size
            - hflip_prob: the probability of horizontally flipping
        &#34;&#34;&#34;
        super(PresetTrain, self).__init__()
        self.task = task

        trans: Sequence[torch.nn.Module] = []
        trans.extend(
            [
                ConvertImageDtype(torch.float),
                RandomAutocaptureCrop(),
                RandomResize(base_size, base_size),
                RandomPerspective(),
                # RandomCrop(int(crop_size * base_size)),
                StrideResize(),
            ]
        )
        if hflip_prob &gt; 0:
            trans.append(RandomHorizontalFlip(hflip_prob))
        if None not in (mean, std):
            trans.append(Normalize(mean, std))

        self.transforms = Compose(trans, task)

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        return self.transforms(image, target)


class PresetEval(torch.nn.Module):
    def __init__(
        self,
        task: str = &#34;segmentation&#34;,
        base_size: int = 320,
        mean: Optional[List[float]] = None,
        std: Optional[List[float]] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - task: the learning task
            - base_size: the desired approximate size of resized images
        &#34;&#34;&#34;
        super(PresetEval, self).__init__()
        self.task = task

        trans: Sequence[torch.nn.Module] = []
        trans.extend(
            [
                ConvertImageDtype(torch.float),
                RandomAutocaptureCrop(
                    mean=torch.zeros(4),
                    variance=(1e-9) * torch.ones(4),
                ),
                RandomResize(base_size, base_size),
                StrideResize(),
            ]
        )
        if None not in (mean, std):
            trans.append(Normalize(mean, std))

        self.transforms = Compose(trans, task)

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        return self.transforms(image, target)


#################### CUSTOM TRANSFORMS ####################
###########################################################


class Compose(torch.nn.Module):
    &#34;&#34;&#34;
    Custom Compose torch.nn.Module

    Apply a list of custom transforms to an image and its segmentation mask target.
    &#34;&#34;&#34;

    def __init__(
        self, transforms: Sequence[torch.nn.Module], task: str = &#34;segmentation&#34;
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - transforms: a list of custom torch.nn.Module transforms
        &#34;&#34;&#34;
        super(Compose, self).__init__()
        self.transforms = transforms
        self.task = task

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        for t in self.transforms:
            image, target = t(image, target)
        return image, target


class RandomResize(torch.nn.Module):
    &#34;&#34;&#34;
    Custom RandomResize torch.nn.Module

    Resize an image and its segmentation mask to a random size in [min_size, max_size).
    If no max_size is provided, no resizing is applied.
    &#34;&#34;&#34;

    def __init__(
        self,
        min_size: int = 224,
        max_size: Optional[int] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - min_size: the minimum of the range of random resized image dimensions
            - max_size: the minimum of the range of random resized image dimensions (if None, no resize is performed)
        &#34;&#34;&#34;
        super(RandomResize, self).__init__()
        self.min_size = min_size
        if max_size is None:
            max_size = min_size
        self.max_size = max_size

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        if self.min_size &lt;= self.max_size:
            if self.min_size == self.max_size:
                size = self.min_size
            else:
                size = int(
                    torch.randint(self.min_size, self.max_size, size=(1,)).item()
                )
            image = F.resize(image, [size, size], antialias=True)
            try:
                target = F.resize(
                    target,
                    [size, size],
                    interpolation=T.InterpolationMode.NEAREST,
                )
            except Exception as e:
                # This allows for the transform to be used on non-mask-style labels (classification probabilities)
                # NOTE: May cause problems if we integrate object detection/instance segmentation
                pass
        return image, target


class StrideResize(torch.nn.Module):
    &#34;&#34;&#34;
    Custom stride-based resize torch.nn.Module

    Resize an image and its segmentation mask such that both height and width are multiples of stride.
    Minimally change the image aspect ratio
    &#34;&#34;&#34;

    def __init__(self, stride: int = 32) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - stride: the stride/layer size interval (32, 64)
        &#34;&#34;&#34;
        super(StrideResize, self).__init__()
        self.stride = stride

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        w, h = image.shape[1:]
        desired_size = [w - (w % self.stride), h - (h % self.stride)]
        image = F.resize(image, desired_size, antialias=True)
        try:
            target = F.resize(
                target,
                desired_size,
                interpolation=T.InterpolationMode.NEAREST,
            )
        except TypeError:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
        return image, target


class RandomCrop(torch.nn.Module):
    &#34;&#34;&#34;
    Custom RandomCrop torch.nn.Module

    Crop an image and its segmentation mask to a specified size.
    The center of the crop is chosen randomly, where zero-padding is used if necessary.
    &#34;&#34;&#34;

    def __init__(self, size: int = 224) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - size: the size of the crop
        &#34;&#34;&#34;
        super(RandomCrop, self).__init__()
        self.size = size
        self.random_crop = T.RandomCrop(size, pad_if_needed=True)

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        crop_params = self.random_crop.get_params(image, (self.size, self.size))
        image = F.crop(image, *crop_params)
        try:
            target = F.crop(target, *crop_params)
        except TypeError:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
        return image, target


class CenterCrop(torch.nn.Module):
    &#34;&#34;&#34;
    Custom CenterCrop torch.nn.Module

    Crop an image and its segmentation mask to a specified size.
    The center of the crop is always the image center, where padding is used if necessary.
    &#34;&#34;&#34;

    def __init__(self, size: int = 224) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - size: the size of the crop
        &#34;&#34;&#34;
        super(CenterCrop, self).__init__()
        self.size = size

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        image = F.center_crop(image, [self.size])
        try:
            target = F.center_crop(target, [self.size])
        except TypeError:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
        return image, target


class RandomHorizontalFlip(torch.nn.Module):
    &#34;&#34;&#34;
    Custom RandomHorizontalFlip torch.nn.Module

    Horizontally flip an image and its segmentation mask with probability flip_prob.
    &#34;&#34;&#34;

    def __init__(self, flip_prob: float = 0.5) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - flip_prob: the probability of the image being flipped
        &#34;&#34;&#34;
        super(RandomHorizontalFlip, self).__init__()
        self.flip_prob = flip_prob

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        if torch.rand(1) &lt; self.flip_prob:
            image = F.hflip(image)
            try:
                target = F.hflip(target)
            except TypeError:
                # This allows for the transform to be used on non-mask-style labels (classification probabilities)
                # NOTE: May cause problems if we integrate object detection/instance segmentation
                pass
        return image, target


class ConvertImageDtype(torch.nn.Module):
    &#34;&#34;&#34;
    Custom ConvertImageDtype torch.nn.Module

    Change the datatype of an image (not applied to its segmentation mask).
    &#34;&#34;&#34;

    def __init__(self, dtype: torch.dtype = torch.float):
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - dtype: the datatype of the image
        &#34;&#34;&#34;
        super(ConvertImageDtype, self).__init__()
        self.dtype = dtype

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        image = F.convert_image_dtype(image, self.dtype)
        return image, target


class Normalize(torch.nn.Module):
    &#34;&#34;&#34;
    Custom Normalize torch.nn.Module

    Normalize an image (not applied to its segmentation mask).
    &#34;&#34;&#34;

    def __init__(
        self,
        mean: Optional[List[float]] = None,
        std: Optional[List[float]] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - mean: the mean of the un-normalized image data
            - std: the standard deviation of the un-normalized image data
        &#34;&#34;&#34;
        super(Normalize, self).__init__()
        self.mean = mean
        self.std = std

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        if self.mean is not None and self.std is not None:
            image = F.normalize(image, mean=self.mean, std=self.std)
        return image, target


class ToTensor(torch.nn.Module):
    &#34;&#34;&#34;
    Custom ToTensor torch.nn.Module

    ToTensor Transform (convienence)
    &#34;&#34;&#34;

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;
        Constructor
        &#34;&#34;&#34;
        super(ToTensor, self).__init__()
        self.to_tensor = T.ToTensor()

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        if not isinstance(image, torch.Tensor):
            # This operation converts ndarrays/PIL Images of integers [0:255] to FloatTensors [0:1]
            image = self.to_tensor(image)
        if target is not None and not isinstance(target, torch.Tensor):
            target = torch.as_tensor(target)
        return image, target


class RandomPerspective(torch.nn.Module):
    &#34;&#34;&#34;
    Custom RandomPerspective torch.nn.Module

    Same behavior as the torchvision transform class of the same name.
    Compatible with segmentation masks.
    &#34;&#34;&#34;

    def __init__(
        self,
        distortion_scale: float = 0.1,
        p: float = 0.75,
        fill: int = 0,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - distortion_scale: the degree of distortion on [0, 1]
            - p: the probability of making a random perspective on [0, 1]
            - fill: the fill value on the edges of the resulting images
        &#34;&#34;&#34;
        super(RandomPerspective, self).__init__()
        self.distortion_scale = distortion_scale
        self.p = p
        self.fill = fill
        self.random_perspective = T.RandomPerspective(distortion_scale, p, fill=fill)

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;

        perspective_params = self.random_perspective.get_params(
            image.shape[-1], image.shape[-2], distortion_scale=self.distortion_scale
        )
        image = F.perspective(image, perspective_params[0], perspective_params[1])
        try:
            target = F.perspective(
                target,
                perspective_params[0],
                perspective_params[1],
                interpolation=T.InterpolationMode.NEAREST,
            )
        except TypeError:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
        return image, target


class RandomAutocaptureCrop(torch.nn.Module):
    &#34;&#34;&#34;
    Custom RandomAutocaptureCrop torch.nn.Module

    Use the detect_eyes function from the autocapture module,
    apply random noise to the (x, y, w, h)-dimensions, and extract the crop.
    &#34;&#34;&#34;

    def __init__(
        self,
        mean: torch.Tensor = torch.tensor([0.0, 0.0, 0.1, 0.05]),
        variance: torch.Tensor = 0.001 * torch.ones(4),
        padding_factor: float = 1.15,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - loc: the center of the gaussian noise to add to the (x, y, w, h)-dimensions
            - variance: the scale of gaussian noise to add to the (x, y, w, h)-dimensions
        &#34;&#34;&#34;
        super(RandomAutocaptureCrop, self).__init__()
        self.noise = torch.distributions.MultivariateNormal(
            loc=mean,
            covariance_matrix=torch.diag(variance),
        )
        self.padding_factor = padding_factor

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        if image.requires_grad:
            # if the tensor requires grad, we should not detach it.
            return image, target

        cv2_frame = (
            (255 * image).numpy().astype(&#34;uint8&#34;).transpose(1, 2, 0)
        )  # torch tensor to cv2 frame (rgb)
        cv2_frame = cv2_frame[..., ::-1]  # rgb to bgr

        height, width = cv2_frame.shape[:-1]
        det, _ = detect_eyes(cv2_frame, padding_factor=self.padding_factor)
        if len(det) &gt; 0:
            # detection is (x_tl, y_tl, w, h), we want to keep the eye centered to we convert to (cx, cy, w, h)
            noisy_det = torch.tensor(det[0])
            noisy_det[:2] += noisy_det[2:] / 2
            # perturb the detection
            noisy_det += self.noise.sample()
            # convert the coordinates back to (x_tl, y_tl, w, h)
            noisy_det[:2] -= noisy_det[2:] / 2
            image = F.crop(
                image,
                left=int(noisy_det[0] * width),
                top=int(noisy_det[1] * height),
                height=int(noisy_det[3] * height),
                width=int(noisy_det[2] * width),
            )
            try:
                target = F.crop(
                    target,
                    left=int(noisy_det[0] * width),
                    top=int(noisy_det[1] * height),
                    height=int(noisy_det[3] * height),
                    width=int(noisy_det[2] * width),
                )
            except TypeError:
                # This allows for the transform to be used on non-mask-style labels (classification probabilities)
                # NOTE: May cause problems if we integrate object detection/instance segmentation
                pass
        return image, target</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="iris.transforms.CenterCrop"><code class="flex name class">
<span>class <span class="ident">CenterCrop</span></span>
<span>(</span><span>size: int = 224)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom CenterCrop torch.nn.Module</p>
<p>Crop an image and its segmentation mask to a specified size.
The center of the crop is always the image center, where padding is used if necessary.</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>size: the size of the crop</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CenterCrop(torch.nn.Module):
    &#34;&#34;&#34;
    Custom CenterCrop torch.nn.Module

    Crop an image and its segmentation mask to a specified size.
    The center of the crop is always the image center, where padding is used if necessary.
    &#34;&#34;&#34;

    def __init__(self, size: int = 224) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - size: the size of the crop
        &#34;&#34;&#34;
        super(CenterCrop, self).__init__()
        self.size = size

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        image = F.center_crop(image, [self.size])
        try:
            target = F.center_crop(target, [self.size])
        except TypeError:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.CenterCrop.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    image = F.center_crop(image, [self.size])
    try:
        target = F.center_crop(target, [self.size])
    except TypeError:
        # This allows for the transform to be used on non-mask-style labels (classification probabilities)
        # NOTE: May cause problems if we integrate object detection/instance segmentation
        pass
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.Compose"><code class="flex name class">
<span>class <span class="ident">Compose</span></span>
<span>(</span><span>transforms: Sequence[torch.nn.modules.module.Module], task: str = 'segmentation')</span>
</code></dt>
<dd>
<div class="desc"><p>Custom Compose torch.nn.Module</p>
<p>Apply a list of custom transforms to an image and its segmentation mask target.</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>transforms: a list of custom torch.nn.Module transforms</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Compose(torch.nn.Module):
    &#34;&#34;&#34;
    Custom Compose torch.nn.Module

    Apply a list of custom transforms to an image and its segmentation mask target.
    &#34;&#34;&#34;

    def __init__(
        self, transforms: Sequence[torch.nn.Module], task: str = &#34;segmentation&#34;
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - transforms: a list of custom torch.nn.Module transforms
        &#34;&#34;&#34;
        super(Compose, self).__init__()
        self.transforms = transforms
        self.task = task

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        for t in self.transforms:
            image, target = t(image, target)
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.Compose.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    for t in self.transforms:
        image, target = t(image, target)
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.ConvertImageDtype"><code class="flex name class">
<span>class <span class="ident">ConvertImageDtype</span></span>
<span>(</span><span>dtype: torch.dtype = torch.float32)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom ConvertImageDtype torch.nn.Module</p>
<p>Change the datatype of an image (not applied to its segmentation mask).</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>dtype: the datatype of the image</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvertImageDtype(torch.nn.Module):
    &#34;&#34;&#34;
    Custom ConvertImageDtype torch.nn.Module

    Change the datatype of an image (not applied to its segmentation mask).
    &#34;&#34;&#34;

    def __init__(self, dtype: torch.dtype = torch.float):
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - dtype: the datatype of the image
        &#34;&#34;&#34;
        super(ConvertImageDtype, self).__init__()
        self.dtype = dtype

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        image = F.convert_image_dtype(image, self.dtype)
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.ConvertImageDtype.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    image = F.convert_image_dtype(image, self.dtype)
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.Normalize"><code class="flex name class">
<span>class <span class="ident">Normalize</span></span>
<span>(</span><span>mean: Optional[List[float]] = None, std: Optional[List[float]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom Normalize torch.nn.Module</p>
<p>Normalize an image (not applied to its segmentation mask).</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>mean: the mean of the un-normalized image data</li>
<li>std: the standard deviation of the un-normalized image data</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Normalize(torch.nn.Module):
    &#34;&#34;&#34;
    Custom Normalize torch.nn.Module

    Normalize an image (not applied to its segmentation mask).
    &#34;&#34;&#34;

    def __init__(
        self,
        mean: Optional[List[float]] = None,
        std: Optional[List[float]] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - mean: the mean of the un-normalized image data
            - std: the standard deviation of the un-normalized image data
        &#34;&#34;&#34;
        super(Normalize, self).__init__()
        self.mean = mean
        self.std = std

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        if self.mean is not None and self.std is not None:
            image = F.normalize(image, mean=self.mean, std=self.std)
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.Normalize.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    if self.mean is not None and self.std is not None:
        image = F.normalize(image, mean=self.mean, std=self.std)
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.PresetEval"><code class="flex name class">
<span>class <span class="ident">PresetEval</span></span>
<span>(</span><span>task: str = 'segmentation', base_size: int = 320, mean: Optional[List[float]] = None, std: Optional[List[float]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>task: the learning task</li>
<li>base_size: the desired approximate size of resized images</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PresetEval(torch.nn.Module):
    def __init__(
        self,
        task: str = &#34;segmentation&#34;,
        base_size: int = 320,
        mean: Optional[List[float]] = None,
        std: Optional[List[float]] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - task: the learning task
            - base_size: the desired approximate size of resized images
        &#34;&#34;&#34;
        super(PresetEval, self).__init__()
        self.task = task

        trans: Sequence[torch.nn.Module] = []
        trans.extend(
            [
                ConvertImageDtype(torch.float),
                RandomAutocaptureCrop(
                    mean=torch.zeros(4),
                    variance=(1e-9) * torch.ones(4),
                ),
                RandomResize(base_size, base_size),
                StrideResize(),
            ]
        )
        if None not in (mean, std):
            trans.append(Normalize(mean, std))

        self.transforms = Compose(trans, task)

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        return self.transforms(image, target)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.PresetEval.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    return self.transforms(image, target)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.PresetTrain"><code class="flex name class">
<span>class <span class="ident">PresetTrain</span></span>
<span>(</span><span>task: str = 'segmentation', base_size: int = 320, crop_size: float = 0.7, hflip_prob: float = 0.5, mean: Optional[List[float]] = None, std: Optional[List[float]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom Composed Transforms for training</p>
<p>Compatible with Segmentation and Classification Models</p>
<p>Training augmentations:</p>
<pre><code>1. ConvertImageDtype(torch.float)
2. RandomAutocaptureCrop()
3. RandomResize(base_size, base_size)
4. RandomPerspective()
5. RandomCrop(int(crop_size * base_size))
6. StrideResize()
7. RandomHorizontalFlip(hflip_prob)
</code></pre>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>task: the learning task</li>
<li>base_size: the desired approximate size of resized images</li>
<li>crop_size: the desired size of cropped images as a percentage of base_size</li>
<li>hflip_prob: the probability of horizontally flipping</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PresetTrain(torch.nn.Module):
    &#34;&#34;&#34;
    Custom Composed Transforms for training

    Compatible with Segmentation and Classification Models

    Training augmentations:

        1. ConvertImageDtype(torch.float)
        2. RandomAutocaptureCrop()
        3. RandomResize(base_size, base_size)
        4. RandomPerspective()
        5. RandomCrop(int(crop_size * base_size))
        6. StrideResize()
        7. RandomHorizontalFlip(hflip_prob)
    &#34;&#34;&#34;

    def __init__(
        self,
        task: str = &#34;segmentation&#34;,
        base_size: int = 320,
        crop_size: float = 0.7,
        hflip_prob: float = 0.5,
        mean: Optional[List[float]] = None,
        std: Optional[List[float]] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - task: the learning task
            - base_size: the desired approximate size of resized images
            - crop_size: the desired size of cropped images as a percentage of base_size
            - hflip_prob: the probability of horizontally flipping
        &#34;&#34;&#34;
        super(PresetTrain, self).__init__()
        self.task = task

        trans: Sequence[torch.nn.Module] = []
        trans.extend(
            [
                ConvertImageDtype(torch.float),
                RandomAutocaptureCrop(),
                RandomResize(base_size, base_size),
                RandomPerspective(),
                # RandomCrop(int(crop_size * base_size)),
                StrideResize(),
            ]
        )
        if hflip_prob &gt; 0:
            trans.append(RandomHorizontalFlip(hflip_prob))
        if None not in (mean, std):
            trans.append(Normalize(mean, std))

        self.transforms = Compose(trans, task)

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        return self.transforms(image, target)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.PresetTrain.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    return self.transforms(image, target)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.RandomAutocaptureCrop"><code class="flex name class">
<span>class <span class="ident">RandomAutocaptureCrop</span></span>
<span>(</span><span>mean: torch.Tensor = tensor([0.0000, 0.0000, 0.1000, 0.0500]), variance: torch.Tensor = tensor([0.0010, 0.0010, 0.0010, 0.0010]), padding_factor: float = 1.15)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom RandomAutocaptureCrop torch.nn.Module</p>
<p>Use the detect_eyes function from the autocapture module,
apply random noise to the (x, y, w, h)-dimensions, and extract the crop.</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>loc: the center of the gaussian noise to add to the (x, y, w, h)-dimensions</li>
<li>variance: the scale of gaussian noise to add to the (x, y, w, h)-dimensions</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RandomAutocaptureCrop(torch.nn.Module):
    &#34;&#34;&#34;
    Custom RandomAutocaptureCrop torch.nn.Module

    Use the detect_eyes function from the autocapture module,
    apply random noise to the (x, y, w, h)-dimensions, and extract the crop.
    &#34;&#34;&#34;

    def __init__(
        self,
        mean: torch.Tensor = torch.tensor([0.0, 0.0, 0.1, 0.05]),
        variance: torch.Tensor = 0.001 * torch.ones(4),
        padding_factor: float = 1.15,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - loc: the center of the gaussian noise to add to the (x, y, w, h)-dimensions
            - variance: the scale of gaussian noise to add to the (x, y, w, h)-dimensions
        &#34;&#34;&#34;
        super(RandomAutocaptureCrop, self).__init__()
        self.noise = torch.distributions.MultivariateNormal(
            loc=mean,
            covariance_matrix=torch.diag(variance),
        )
        self.padding_factor = padding_factor

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        if image.requires_grad:
            # if the tensor requires grad, we should not detach it.
            return image, target

        cv2_frame = (
            (255 * image).numpy().astype(&#34;uint8&#34;).transpose(1, 2, 0)
        )  # torch tensor to cv2 frame (rgb)
        cv2_frame = cv2_frame[..., ::-1]  # rgb to bgr

        height, width = cv2_frame.shape[:-1]
        det, _ = detect_eyes(cv2_frame, padding_factor=self.padding_factor)
        if len(det) &gt; 0:
            # detection is (x_tl, y_tl, w, h), we want to keep the eye centered to we convert to (cx, cy, w, h)
            noisy_det = torch.tensor(det[0])
            noisy_det[:2] += noisy_det[2:] / 2
            # perturb the detection
            noisy_det += self.noise.sample()
            # convert the coordinates back to (x_tl, y_tl, w, h)
            noisy_det[:2] -= noisy_det[2:] / 2
            image = F.crop(
                image,
                left=int(noisy_det[0] * width),
                top=int(noisy_det[1] * height),
                height=int(noisy_det[3] * height),
                width=int(noisy_det[2] * width),
            )
            try:
                target = F.crop(
                    target,
                    left=int(noisy_det[0] * width),
                    top=int(noisy_det[1] * height),
                    height=int(noisy_det[3] * height),
                    width=int(noisy_det[2] * width),
                )
            except TypeError:
                # This allows for the transform to be used on non-mask-style labels (classification probabilities)
                # NOTE: May cause problems if we integrate object detection/instance segmentation
                pass
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.RandomAutocaptureCrop.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    if image.requires_grad:
        # if the tensor requires grad, we should not detach it.
        return image, target

    cv2_frame = (
        (255 * image).numpy().astype(&#34;uint8&#34;).transpose(1, 2, 0)
    )  # torch tensor to cv2 frame (rgb)
    cv2_frame = cv2_frame[..., ::-1]  # rgb to bgr

    height, width = cv2_frame.shape[:-1]
    det, _ = detect_eyes(cv2_frame, padding_factor=self.padding_factor)
    if len(det) &gt; 0:
        # detection is (x_tl, y_tl, w, h), we want to keep the eye centered to we convert to (cx, cy, w, h)
        noisy_det = torch.tensor(det[0])
        noisy_det[:2] += noisy_det[2:] / 2
        # perturb the detection
        noisy_det += self.noise.sample()
        # convert the coordinates back to (x_tl, y_tl, w, h)
        noisy_det[:2] -= noisy_det[2:] / 2
        image = F.crop(
            image,
            left=int(noisy_det[0] * width),
            top=int(noisy_det[1] * height),
            height=int(noisy_det[3] * height),
            width=int(noisy_det[2] * width),
        )
        try:
            target = F.crop(
                target,
                left=int(noisy_det[0] * width),
                top=int(noisy_det[1] * height),
                height=int(noisy_det[3] * height),
                width=int(noisy_det[2] * width),
            )
        except TypeError:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.RandomCrop"><code class="flex name class">
<span>class <span class="ident">RandomCrop</span></span>
<span>(</span><span>size: int = 224)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom RandomCrop torch.nn.Module</p>
<p>Crop an image and its segmentation mask to a specified size.
The center of the crop is chosen randomly, where zero-padding is used if necessary.</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>size: the size of the crop</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RandomCrop(torch.nn.Module):
    &#34;&#34;&#34;
    Custom RandomCrop torch.nn.Module

    Crop an image and its segmentation mask to a specified size.
    The center of the crop is chosen randomly, where zero-padding is used if necessary.
    &#34;&#34;&#34;

    def __init__(self, size: int = 224) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - size: the size of the crop
        &#34;&#34;&#34;
        super(RandomCrop, self).__init__()
        self.size = size
        self.random_crop = T.RandomCrop(size, pad_if_needed=True)

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        crop_params = self.random_crop.get_params(image, (self.size, self.size))
        image = F.crop(image, *crop_params)
        try:
            target = F.crop(target, *crop_params)
        except TypeError:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.RandomCrop.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    crop_params = self.random_crop.get_params(image, (self.size, self.size))
    image = F.crop(image, *crop_params)
    try:
        target = F.crop(target, *crop_params)
    except TypeError:
        # This allows for the transform to be used on non-mask-style labels (classification probabilities)
        # NOTE: May cause problems if we integrate object detection/instance segmentation
        pass
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.RandomHorizontalFlip"><code class="flex name class">
<span>class <span class="ident">RandomHorizontalFlip</span></span>
<span>(</span><span>flip_prob: float = 0.5)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom RandomHorizontalFlip torch.nn.Module</p>
<p>Horizontally flip an image and its segmentation mask with probability flip_prob.</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>flip_prob: the probability of the image being flipped</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RandomHorizontalFlip(torch.nn.Module):
    &#34;&#34;&#34;
    Custom RandomHorizontalFlip torch.nn.Module

    Horizontally flip an image and its segmentation mask with probability flip_prob.
    &#34;&#34;&#34;

    def __init__(self, flip_prob: float = 0.5) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - flip_prob: the probability of the image being flipped
        &#34;&#34;&#34;
        super(RandomHorizontalFlip, self).__init__()
        self.flip_prob = flip_prob

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        if torch.rand(1) &lt; self.flip_prob:
            image = F.hflip(image)
            try:
                target = F.hflip(target)
            except TypeError:
                # This allows for the transform to be used on non-mask-style labels (classification probabilities)
                # NOTE: May cause problems if we integrate object detection/instance segmentation
                pass
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.RandomHorizontalFlip.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    if torch.rand(1) &lt; self.flip_prob:
        image = F.hflip(image)
        try:
            target = F.hflip(target)
        except TypeError:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.RandomPerspective"><code class="flex name class">
<span>class <span class="ident">RandomPerspective</span></span>
<span>(</span><span>distortion_scale: float = 0.1, p: float = 0.75, fill: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom RandomPerspective torch.nn.Module</p>
<p>Same behavior as the torchvision transform class of the same name.
Compatible with segmentation masks.</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>distortion_scale: the degree of distortion on [0, 1]</li>
<li>p: the probability of making a random perspective on [0, 1]</li>
<li>fill: the fill value on the edges of the resulting images</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RandomPerspective(torch.nn.Module):
    &#34;&#34;&#34;
    Custom RandomPerspective torch.nn.Module

    Same behavior as the torchvision transform class of the same name.
    Compatible with segmentation masks.
    &#34;&#34;&#34;

    def __init__(
        self,
        distortion_scale: float = 0.1,
        p: float = 0.75,
        fill: int = 0,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - distortion_scale: the degree of distortion on [0, 1]
            - p: the probability of making a random perspective on [0, 1]
            - fill: the fill value on the edges of the resulting images
        &#34;&#34;&#34;
        super(RandomPerspective, self).__init__()
        self.distortion_scale = distortion_scale
        self.p = p
        self.fill = fill
        self.random_perspective = T.RandomPerspective(distortion_scale, p, fill=fill)

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;

        perspective_params = self.random_perspective.get_params(
            image.shape[-1], image.shape[-2], distortion_scale=self.distortion_scale
        )
        image = F.perspective(image, perspective_params[0], perspective_params[1])
        try:
            target = F.perspective(
                target,
                perspective_params[0],
                perspective_params[1],
                interpolation=T.InterpolationMode.NEAREST,
            )
        except TypeError:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.RandomPerspective.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;

    perspective_params = self.random_perspective.get_params(
        image.shape[-1], image.shape[-2], distortion_scale=self.distortion_scale
    )
    image = F.perspective(image, perspective_params[0], perspective_params[1])
    try:
        target = F.perspective(
            target,
            perspective_params[0],
            perspective_params[1],
            interpolation=T.InterpolationMode.NEAREST,
        )
    except TypeError:
        # This allows for the transform to be used on non-mask-style labels (classification probabilities)
        # NOTE: May cause problems if we integrate object detection/instance segmentation
        pass
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.RandomResize"><code class="flex name class">
<span>class <span class="ident">RandomResize</span></span>
<span>(</span><span>min_size: int = 224, max_size: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom RandomResize torch.nn.Module</p>
<p>Resize an image and its segmentation mask to a random size in [min_size, max_size).
If no max_size is provided, no resizing is applied.</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>min_size: the minimum of the range of random resized image dimensions</li>
<li>max_size: the minimum of the range of random resized image dimensions (if None, no resize is performed)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RandomResize(torch.nn.Module):
    &#34;&#34;&#34;
    Custom RandomResize torch.nn.Module

    Resize an image and its segmentation mask to a random size in [min_size, max_size).
    If no max_size is provided, no resizing is applied.
    &#34;&#34;&#34;

    def __init__(
        self,
        min_size: int = 224,
        max_size: Optional[int] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - min_size: the minimum of the range of random resized image dimensions
            - max_size: the minimum of the range of random resized image dimensions (if None, no resize is performed)
        &#34;&#34;&#34;
        super(RandomResize, self).__init__()
        self.min_size = min_size
        if max_size is None:
            max_size = min_size
        self.max_size = max_size

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        if self.min_size &lt;= self.max_size:
            if self.min_size == self.max_size:
                size = self.min_size
            else:
                size = int(
                    torch.randint(self.min_size, self.max_size, size=(1,)).item()
                )
            image = F.resize(image, [size, size], antialias=True)
            try:
                target = F.resize(
                    target,
                    [size, size],
                    interpolation=T.InterpolationMode.NEAREST,
                )
            except Exception as e:
                # This allows for the transform to be used on non-mask-style labels (classification probabilities)
                # NOTE: May cause problems if we integrate object detection/instance segmentation
                pass
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.RandomResize.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    if self.min_size &lt;= self.max_size:
        if self.min_size == self.max_size:
            size = self.min_size
        else:
            size = int(
                torch.randint(self.min_size, self.max_size, size=(1,)).item()
            )
        image = F.resize(image, [size, size], antialias=True)
        try:
            target = F.resize(
                target,
                [size, size],
                interpolation=T.InterpolationMode.NEAREST,
            )
        except Exception as e:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.StrideResize"><code class="flex name class">
<span>class <span class="ident">StrideResize</span></span>
<span>(</span><span>stride: int = 32)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom stride-based resize torch.nn.Module</p>
<p>Resize an image and its segmentation mask such that both height and width are multiples of stride.
Minimally change the image aspect ratio</p>
<p>Constructor</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>stride: the stride/layer size interval (32, 64)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StrideResize(torch.nn.Module):
    &#34;&#34;&#34;
    Custom stride-based resize torch.nn.Module

    Resize an image and its segmentation mask such that both height and width are multiples of stride.
    Minimally change the image aspect ratio
    &#34;&#34;&#34;

    def __init__(self, stride: int = 32) -&gt; None:
        &#34;&#34;&#34;
        Constructor

        Arguments:
            - stride: the stride/layer size interval (32, 64)
        &#34;&#34;&#34;
        super(StrideResize, self).__init__()
        self.stride = stride

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        w, h = image.shape[1:]
        desired_size = [w - (w % self.stride), h - (h % self.stride)]
        image = F.resize(image, desired_size, antialias=True)
        try:
            target = F.resize(
                target,
                desired_size,
                interpolation=T.InterpolationMode.NEAREST,
            )
        except TypeError:
            # This allows for the transform to be used on non-mask-style labels (classification probabilities)
            # NOTE: May cause problems if we integrate object detection/instance segmentation
            pass
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.StrideResize.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    w, h = image.shape[1:]
    desired_size = [w - (w % self.stride), h - (h % self.stride)]
    image = F.resize(image, desired_size, antialias=True)
    try:
        target = F.resize(
            target,
            desired_size,
            interpolation=T.InterpolationMode.NEAREST,
        )
    except TypeError:
        # This allows for the transform to be used on non-mask-style labels (classification probabilities)
        # NOTE: May cause problems if we integrate object detection/instance segmentation
        pass
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.transforms.ToTensor"><code class="flex name class">
<span>class <span class="ident">ToTensor</span></span>
</code></dt>
<dd>
<div class="desc"><p>Custom ToTensor torch.nn.Module</p>
<p>ToTensor Transform (convienence)</p>
<p>Constructor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ToTensor(torch.nn.Module):
    &#34;&#34;&#34;
    Custom ToTensor torch.nn.Module

    ToTensor Transform (convienence)
    &#34;&#34;&#34;

    def __init__(self) -&gt; None:
        &#34;&#34;&#34;
        Constructor
        &#34;&#34;&#34;
        super(ToTensor, self).__init__()
        self.to_tensor = T.ToTensor()

    def forward(
        self, image: torch.Tensor, target: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Forward

        Arguments:
            - image: the image (B x C x H x W)
            - target: the segmentation mask (B x 1 x H x W)

        Returns:
            - image: the transformed image (B x C x H x W)
            - target: the transformed segmentation mask (B x 1 x H x W)
        &#34;&#34;&#34;
        if not isinstance(image, torch.Tensor):
            # This operation converts ndarrays/PIL Images of integers [0:255] to FloatTensors [0:1]
            image = self.to_tensor(image)
        if target is not None and not isinstance(target, torch.Tensor):
            target = torch.as_tensor(target)
        return image, target</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.transforms.ToTensor.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, image: torch.Tensor, target: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>image: the image (B x C x H x W)</li>
<li>target: the segmentation mask (B x 1 x H x W)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>image: the transformed image (B x C x H x W)</li>
<li>target: the transformed segmentation mask (B x 1 x H x W)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, image: torch.Tensor, target: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Forward

    Arguments:
        - image: the image (B x C x H x W)
        - target: the segmentation mask (B x 1 x H x W)

    Returns:
        - image: the transformed image (B x C x H x W)
        - target: the transformed segmentation mask (B x 1 x H x W)
    &#34;&#34;&#34;
    if not isinstance(image, torch.Tensor):
        # This operation converts ndarrays/PIL Images of integers [0:255] to FloatTensors [0:1]
        image = self.to_tensor(image)
    if target is not None and not isinstance(target, torch.Tensor):
        target = torch.as_tensor(target)
    return image, target</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="iris" href="index.html">iris</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="iris.transforms.CenterCrop" href="#iris.transforms.CenterCrop">CenterCrop</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.CenterCrop.forward" href="#iris.transforms.CenterCrop.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.Compose" href="#iris.transforms.Compose">Compose</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.Compose.forward" href="#iris.transforms.Compose.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.ConvertImageDtype" href="#iris.transforms.ConvertImageDtype">ConvertImageDtype</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.ConvertImageDtype.forward" href="#iris.transforms.ConvertImageDtype.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.Normalize" href="#iris.transforms.Normalize">Normalize</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.Normalize.forward" href="#iris.transforms.Normalize.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.PresetEval" href="#iris.transforms.PresetEval">PresetEval</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.PresetEval.forward" href="#iris.transforms.PresetEval.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.PresetTrain" href="#iris.transforms.PresetTrain">PresetTrain</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.PresetTrain.forward" href="#iris.transforms.PresetTrain.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.RandomAutocaptureCrop" href="#iris.transforms.RandomAutocaptureCrop">RandomAutocaptureCrop</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.RandomAutocaptureCrop.forward" href="#iris.transforms.RandomAutocaptureCrop.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.RandomCrop" href="#iris.transforms.RandomCrop">RandomCrop</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.RandomCrop.forward" href="#iris.transforms.RandomCrop.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.RandomHorizontalFlip" href="#iris.transforms.RandomHorizontalFlip">RandomHorizontalFlip</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.RandomHorizontalFlip.forward" href="#iris.transforms.RandomHorizontalFlip.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.RandomPerspective" href="#iris.transforms.RandomPerspective">RandomPerspective</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.RandomPerspective.forward" href="#iris.transforms.RandomPerspective.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.RandomResize" href="#iris.transforms.RandomResize">RandomResize</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.RandomResize.forward" href="#iris.transforms.RandomResize.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.StrideResize" href="#iris.transforms.StrideResize">StrideResize</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.StrideResize.forward" href="#iris.transforms.StrideResize.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.transforms.ToTensor" href="#iris.transforms.ToTensor">ToTensor</a></code></h4>
<ul class="">
<li><code><a title="iris.transforms.ToTensor.forward" href="#iris.transforms.ToTensor.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>