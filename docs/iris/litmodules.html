<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>iris.litmodules API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>iris.litmodules</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#################### IMPORTS ####################
#################################################

import sys
from typing import Any, Dict, Optional, OrderedDict, Tuple, Union

import torch
from lightning.pytorch import LightningModule
from torchmetrics.classification import (
    MulticlassAccuracy,
    MulticlassF1Score,
    MulticlassJaccardIndex,
    MultilabelAccuracy,
    MultilabelF1Score,
)
from torchvision import models as torch_models
from torchvision import transforms as T

from iris import criteria as iris_criteria

#################### BASE CUSTOM LIGHTNING MODULE ####################
######################################################################


class IrisLitModule(LightningModule):
    &#34;&#34;&#34;
    Base Iris LightningModule

    Aiming for compatibility/extensibility with:

    Semantic Segmentation - DONE \n
    Instance Segmentation - Use YOLO? \n
    Binary Classification - TODO \n
    Multi-class Classification - DONE \n
    Multi-label Binary-Classification - DONE \n
    &#34;&#34;&#34;

    def __init__(self, cfg: Dict[str, Any]) -&gt; None:
        super().__init__()

        # store hyperparameters in self.hparams
        self.save_hyperparameters(cfg)

        # initialize torch network
        self.model = self._get_torch_network()

        # loss function and loss tracking
        try:
            self.criterion = getattr(torch.nn, self.hparams.criterion)()  # type: ignore
        except Exception as e:
            self.criterion = getattr(iris_criteria, self.hparams.criterion)()  # type: ignore
        self.loss_dict = {&#34;train&#34;: [], &#34;val&#34;: [], &#34;test&#34;: []}

        # for ModelSummary Callback
        self.example_input_array = torch.randn(
            2,
            *self.hparams.imgsz,  # type: ignore
        )

        # for logging images
        self.arr2pil = T.ToPILImage()
        self.log_batches = []

    def forward(self, samples):
        return self.model(samples)

    #################### TRAINING ####################

    def training_step(
        self, batch: Tuple, batch_idx: int
    ) -&gt; Optional[Union[torch.Tensor, Dict[str, Any]]]:
        samples, targets = batch
        preds = self(samples)
        loss = self._lightning_step(&#34;train&#34;, samples, targets, preds)
        return loss

    #################### VALIDATING ####################

    def validation_step(
        self, batch: Tuple, batch_idx: int
    ) -&gt; Optional[Union[torch.Tensor, Dict[str, Any]]]:
        samples, targets = batch
        preds = self(samples)
        loss = self._lightning_step(&#34;val&#34;, samples, targets, preds)
        return loss

    #################### TESTING ####################

    def test_step(
        self, batch: Tuple, batch_idx: int
    ) -&gt; Optional[Union[torch.Tensor, Dict[str, Any]]]:
        samples, targets = batch
        preds = self(samples)
        loss = self._lightning_step(&#34;test&#34;, samples, targets, preds)
        return loss

    def on_test_epoch_end(self) -&gt; None:
        self._log_images()
        self.log_batches = []

    #################### PREDICTION ####################

    def predict_step(self, batch: Tuple, batch_idx: int) -&gt; torch.Tensor:
        samples = batch[0]
        preds = self(samples)
        # semantic segmentation models return dictionaries w/ &#34;out&#34; as the mask key
        if isinstance(preds, OrderedDict):
            return preds[&#34;out&#34;]
        else:
            return preds

    #################### OPTIMIZERS ####################

    def configure_optimizers(self) -&gt; Dict[str, Any]:
        optimizer = self.hparams.optimizer  # type: ignore
        lr = self.hparams.lr  # type: ignore
        weight_decay = self.hparams.weight_decay  # type: ignore

        if optimizer in (&#34;SGD&#34;, &#34;Adadelta&#34;):
            betas = self.hparams.momentum  # type: ignore
        if optimizer in (
            &#34;Adam&#34;,
            &#34;AdamW&#34;,
            &#34;Adamax&#34;,
        ):
            betas = (self.hparams.momentum, 0.999)  # type: ignore

        opt = getattr(torch.optim, self.hparams.optimizer)(  # type: ignore
            self.trainer.model.parameters(), lr, betas, weight_decay=weight_decay  # type: ignore
        )

        opts = {&#34;optimizer&#34;: opt}

        if self.hparams.scheduler == &#34;swa&#34;:  # type: ignore
            pass
        elif self.hparams.scheduler == &#34;linear_warmup_cosine_annealing&#34;:  # type: ignore
            #### Cosine Annealing LR with warmup (use LR finder suggestion)
            T_max = self.hparams.epochs * self.hparams.num_batches  # type: ignore
            T_0 = max(5 * self.hparams.num_batches, int(T_max / 10))  # type: ignore
            start_factor = 1e-2
            warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
                optimizer=opt, start_factor=start_factor, total_iters=T_0
            )
            annealing_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer=opt,
                T_max=T_max - T_0,
                eta_min=start_factor * self.hparams.lr,  # type: ignore
            )
            scheduler_config = {
                &#34;scheduler&#34;: torch.optim.lr_scheduler.SequentialLR(
                    opt,
                    schedulers=[warmup_scheduler, annealing_scheduler],
                    milestones=[T_0 + 1],
                ),
                &#34;interval&#34;: &#34;step&#34;,
                &#34;frequency&#34;: 1,
                &#34;monitor&#34;: &#34;val/loss_epoch&#34;,
                &#34;name&#34;: &#34;scheduler&#34;,
            }
            opts[&#34;lr_scheduler&#34;] = scheduler_config
        else:
            #### Adaptive LR reduction
            patience = max(5, int(0.05 * self.hparams.epochs))  # type: ignore
            scheduler_config = {
                &#34;scheduler&#34;: torch.optim.lr_scheduler.ReduceLROnPlateau(
                    opt,
                    mode=&#34;min&#34;,
                    factor=0.25,
                    patience=patience,
                    threshold=1e-3,
                    threshold_mode=&#34;rel&#34;,
                    cooldown=patience,
                ),
                &#34;interval&#34;: &#34;epoch&#34;,
                &#34;frequency&#34;: 1,
                &#34;monitor&#34;: &#34;val/loss_epoch&#34;,
                &#34;name&#34;: &#34;scheduler&#34;,
            }
            opts[&#34;lr_scheduler&#34;] = scheduler_config
        return opts

    #################### CUSTOM UTILITIES ####################

    def _lightning_step(
        self,
        mode: str,
        samples: torch.Tensor,
        targets: torch.Tensor,
        preds: torch.Tensor,
    ) -&gt; Optional[torch.Tensor]:
        &#34;&#34;&#34;
        General Lightning Step

        mode behavior:
            - &#34;train&#34;: Loss logging
            - &#34;val&#34;: Loss and Metric logging
            - &#34;test&#34; Loss, Metric, and Image logging
            - &#34;predict&#34; Online/Batch Inference API

        Arguments:
            - mode: the string indicating the type of lightning step to perform [&#34;train&#34;, &#34;val&#34;, &#34;test&#34;, &#34;predict&#34;]
            - samples: the batch of samples
            - targets: the batch of targets
            - preds: the batch of predictions

        Returns:
            - loss: the loss as a tensor, often singleton
        &#34;&#34;&#34;

        if mode in (&#34;train&#34;, &#34;val&#34;, &#34;test&#34;):
            loss = self._compute_loss(preds, targets)
            self.log(
                f&#34;{mode}/loss&#34;,
                loss,
                prog_bar=True,
                on_epoch=True,
                on_step=True,
                sync_dist=True,
            )

            if mode in (&#34;val&#34;, &#34;test&#34;):
                # log any attributes of self that start with &#34;metric_XXX&#34;, i.e. self.metric_mIoU
                self_attrs = dir(self)
                for attr in self_attrs:
                    if &#34;metric_&#34; in attr and attr != &#34;_metric_attributes&#34;:
                        # wandb logger errors if we call .log() with a dict
                        metric_or_dict = getattr(self, attr)(preds, targets)
                        if isinstance(metric_or_dict, dict):
                            self.log_dict(
                                {f&#34;{mode}/{k}&#34;: v for k, v in metric_or_dict.items()},
                                on_epoch=True,
                                sync_dist=True,
                            )
                        else:
                            self.log(
                                f&#34;{mode}/{attr.replace(&#39;metric_&#39;, &#39;&#39;)}&#34;,
                                metric_or_dict,
                                on_epoch=True,
                                sync_dist=True,
                            )
                if mode == &#34;test&#34; and len(self.log_batches) &lt; 5:
                    # log images
                    log_batch = {
                        &#34;samples&#34;: samples.detach().cpu(),
                        &#34;ground_truth&#34;: targets.detach().cpu(),
                    }
                    # semantic segmentation models return dictionaries w/ &#34;out&#34; as the mask key
                    if isinstance(preds, OrderedDict):
                        log_batch[&#34;predictions&#34;] = preds[&#34;out&#34;].detach().cpu()  # type: ignore
                    else:
                        log_batch[&#34;predictions&#34;] = preds.detach().cpu()
                    self.log_batches.append(log_batch)
            if loss.isnan().sum() == 0:
                return loss
            else:
                return

    def _get_torch_network(self) -&gt; torch.nn.Module:
        &#34;&#34;&#34;
        Retrieve a network from the torchvision model registry

        Returns:
            - model: a torch.nn.Module object
        &#34;&#34;&#34;
        # torchvision models
        model = torch_models.get_model(
            self.hparams.model_arch,  # type: ignore
            weights=&#34;DEFAULT&#34; if self.hparams.pretrained else None,  # type: ignore
        )

        # Determine the input space of the pytorch default final layer
        model_childrens = [name for name, _ in model.named_children()]
        try:
            final_layer_in_features = getattr(model, f&#34;{model_childrens[-1]}&#34;)[
                -1
            ].in_features
        except Exception as e:
            final_layer_in_features = getattr(
                model, f&#34;{model_childrens[-1]}&#34;
            ).in_features
        # Replace the default output layer with a custom one
        new_output_layer = torch.nn.Linear(
            in_features=final_layer_in_features, out_features=self.hparams.num_classes  # type: ignore
        )
        try:
            getattr(model, f&#34;{model_childrens[-1]}&#34;)[-1] = new_output_layer
        except Exception as e:
            setattr(model, model_childrens[-1], new_output_layer)

        return model

    def _compute_loss(
        self,
        outputs: Union[torch.Tensor, Dict[str, torch.Tensor]],
        targets: torch.Tensor,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Compute loss, defined differently for different tasks

        Arguments:
            - outputs: the outputs of the torch.nn.Module object
            - targets: the ground truth
        Returns:
            - loss: the loss tensor (often singleton)
        &#34;&#34;&#34;
        return self.criterion(outputs, targets)

    def _log_images(self, captions_batch=None, masks_batch=None) -&gt; None:
        &#34;&#34;&#34;
        Function to perform image data logging.
        &#34;&#34;&#34;
        for i, log_batch in enumerate(self.log_batches):
            samples = [self.arr2pil(sample) for sample in log_batch[&#34;samples&#34;]]
            if captions_batch is not None and masks_batch is not None:
                # both captions and masks
                self.logger.log_image(  # type: ignore
                    &#34;test/image&#34;,
                    samples,
                    caption=captions_batch[i],
                    masks=masks_batch[i],
                )
            elif captions_batch is not None:
                # captions only
                self.logger.log_image(&#34;test/image&#34;, samples, caption=captions_batch[i])  # type: ignore
            elif masks_batch is not None:
                # masks only
                self.logger.log_image(&#34;test/image&#34;, samples, masks=masks_batch[i])  # type: ignore
            else:
                # just images
                self.logger.log_image(&#34;test/image&#34;, samples)  # type: ignore


#################### MULTI-CLASS CLASSIFICATION CUSTOM LIGHTNING MODULE ####################
############################################################################################


class MultiClassClassificationIrisLitModule(IrisLitModule):
    &#34;&#34;&#34;
    Multi-class Classification Iris LightningModule
    &#34;&#34;&#34;

    def __init__(self, cfg: Dict[str, Any]) -&gt; None:
        super().__init__(cfg)
        self.metric_acc = MulticlassAccuracy(self.hparams.num_classes)  # type: ignore
        self.metric_f1 = MulticlassF1Score(self.hparams.num_classes)  # type: ignore

    #################### CUSTOM UTILITIES ####################

    def _log_images(self):
        &#34;&#34;&#34;
        Log images with classification targets and predictions as image captions
        &#34;&#34;&#34;
        captions_batch = []
        for log_batch in self.log_batches:
            captions = [
                f&#34;prediction: {self.hparams.classes[torch.argmax(pred, dim=0).item()]}, \n&#34; + # type: ignore
                f&#34;target: {self.hparams.classes[target.item()]}&#34;  # type: ignore
                for pred, target in zip(
                    log_batch[&#34;predictions&#34;], log_batch[&#34;ground_truth&#34;]
                )
            ]
            captions_batch.append(captions)

        super()._log_images(captions_batch=captions_batch)


#################### MULTI-LABEL BINARY CLASSIFICATION LIGHTNING MODULE ####################
############################################################################################


class MultiLabelClassificationIrisLitModule(IrisLitModule):
    &#34;&#34;&#34;
    Multi-Label Binary Classification Iris LightningModule
    &#34;&#34;&#34;

    def __init__(self, cfg: Dict[str, Any]) -&gt; None:
        super().__init__(cfg)
        self._accuracy = MultilabelAccuracy(num_labels=self.hparams.num_classes, average=None)  # type: ignore
        self.metric_acc = lambda preds, targets: {
            f&#34;acc_{i}&#34;: acc_i.item()
            for i, acc_i in enumerate(self._accuracy(preds, targets))
        }
        self.metric_f1 = MultilabelF1Score(num_labels=self.hparams.num_classes)  # type: ignore

    #################### CUSTOM UTILITIES ####################

    def _log_images(self):
        &#34;&#34;&#34;
        Log images with classification targets and predictions as image captions
        &#34;&#34;&#34;
        captions_batch = []
        for log_batch in self.log_batches:
            captions = [
                f&#34;prediction: {[ self.hparams.classes[i] + &#39;(&#39; + str(round(elem, 3)) + &#39;)&#39; for i, elem in enumerate(pred.tolist()) if (elem &gt; 0.5) ]}, \n&#34; +  # type: ignore
                f&#34;target: {[ self.hparams.classes[i] for i, elem in enumerate(target.int().tolist()) if (elem &gt; 0.5) ]}&#34;  # type: ignore
                for pred, target in zip(
                    log_batch[&#34;predictions&#34;], log_batch[&#34;ground_truth&#34;]
                )
            ]
            captions_batch.append(captions)

        super()._log_images(captions_batch=captions_batch)


#################### SEMANTIC SEGMENTATION CUSTOM LIGHTNING MODULE ####################
#######################################################################################


class SemanticSegmentationIrisLitModule(IrisLitModule):
    &#34;&#34;&#34;
    Semantic Segmentation Iris LightningModule (inherited from Base Iris LitModule)
    &#34;&#34;&#34;

    def __init__(self, cfg: Dict[str, Any]) -&gt; None:
        if &#34;ignore_index&#34; not in cfg.keys():
            cfg[&#34;ignore_index&#34;] = -100
        super().__init__(cfg)
        # loss function and loss tracking (ignoring index 0 bc it is the background)
        try:
            self.criterion = getattr(torch.nn, self.hparams.criterion)(ignore_index=self.hparams.ignore_index)  # type: ignore
        except Exception as e:
            self.criterion = getattr(iris_criteria, self.hparams.criterion)()  # type: ignore
        # Multiclass Accuracy and Multiclass Jaccard Index as performance metrics
        self._accuracy = MulticlassAccuracy(self.hparams.num_classes, ignore_index=self.hparams.ignore_index)  # type: ignore
        self._mIoU = MulticlassJaccardIndex(self.hparams.num_classes, ignore_index=self.hparams.ignore_index)  # type: ignore
        self.metric_acc = lambda preds, targets: self._accuracy(
            preds[&#34;out&#34;], targets.squeeze(1)
        )
        self.metric_mIoU = lambda preds, targets: self._mIoU(
            preds[&#34;out&#34;], targets.squeeze(1)
        )

    #################### CUSTOM UTILITIES ####################

    def _get_torch_network(self) -&gt; torch.nn.Module:
        &#34;&#34;&#34;
        Retrieve a network, either custom or torchvision

        Available Semantic Segmentation Models (self.cfg[&#34;model_arch&#34;]):
            - RITNET (custom)
            - torchvision.models.segmentation:
                - fcn_resnet50, fcn_resnet101
                - deeplabv3_mobilenet_v3_large, deeplabv3_resnet50, deeplabv3_resnet101
                - lraspp_mobilenet_v3_large

        Returns:
            - model: a torch.nn.Module object
        &#34;&#34;&#34;

        # torchvision models
        weights_backbone = &#34;DEFAULT&#34; if self.hparams.pretrained else None  # type: ignore
        try:
            model = torch_models.get_model(
                self.hparams.model_arch,  # type: ignore
                weights_backbone=weights_backbone,
                num_classes=self.hparams.num_classes,  # type: ignore
                aux_loss=True,
            )
        except Exception as e:
            try:
                model = torch_models.get_model(
                    self.hparams.model_arch,  # type: ignore
                    weights_backbone=weights_backbone,
                    num_classes=self.hparams.num_classes,  # type: ignore
                    aux_loss=False,
                )
            except Exception as e:
                print(&#34;Model Architecture not found&#34;)
                sys.exit(1)
        return model

    def _compute_loss(
        self, outputs: Dict[str, torch.Tensor], targets: torch.Tensor
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Compute loss for a segmentation network

        Arguments:
            - outputs: the outputs of the torch.nn.Module object
            - targets: the ground truth
        Returns:
            - loss: the loss tensor (often singleton)
        &#34;&#34;&#34;
        losses = {}
        for name, x in outputs.items():
            losses[name] = self.criterion(x, targets.squeeze(1))

        if len(losses) == 1:
            return losses[&#34;out&#34;]

        return losses[&#34;out&#34;] + 0.5 * losses[&#34;aux&#34;]

    def _log_images(self):
        &#34;&#34;&#34;
        Log images with predicted segmentation masks.
        &#34;&#34;&#34;
        masks_batch = []
        for log_batch in self.log_batches:
            masks = [
                {
                    &#34;predictions&#34;: {
                        &#34;mask_data&#34;: torch.argmax(pred, dim=0).numpy(),
                        # &#34;class_labels&#34;: class_labels,
                    },
                    &#34;ground_truth&#34;: {
                        &#34;mask_data&#34;: torch.squeeze(target, 0).int().numpy(),
                        # &#34;class_labels&#34;: class_labels,
                    },
                }
                for pred, target in zip(
                    log_batch[&#34;predictions&#34;], log_batch[&#34;ground_truth&#34;]
                )
            ]
            masks_batch.append(masks)

        super()._log_images(masks_batch=masks_batch)


#################### MODEL RETRIEVAL UTILITIES ####################
###################################################################


def get_model(
    cfg: Dict[str, Any], model_root: Optional[str] = None
) -&gt; IrisLitModule:
    &#34;&#34;&#34;
    Get a model, optionally by checkpoint

    Arguments:
        - cfg: the primary model/training/data config
        - model_root: (optional) the path to a model.ckpt file

    Returns:
        - lit_module: a IrisLitModule object
    &#34;&#34;&#34;
    task_litmodule_map = {
        &#34;segmentation&#34;: SemanticSegmentationIrisLitModule,
        &#34;classification&#34;: MultiClassClassificationIrisLitModule,
        &#34;multilabel&#34;: MultiLabelClassificationIrisLitModule,
    }
    if model_root is None:
        # load fresh model
        return task_litmodule_map[cfg[&#34;task&#34;]](cfg)
    else:
        try:
            # load a checkpoint
            map_location = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;
            return task_litmodule_map[cfg[&#34;task&#34;]].load_from_checkpoint(
                f&#34;{model_root}/model.ckpt&#34;,
                map_location=map_location,
                cfg=cfg,
                strict=False,
            )
        except Exception as e:
            print(e)
            print(
                &#34;Checkpoint likely not found, returning requested model architecture (untrained)&#34;
            )
            return task_litmodule_map[cfg[&#34;task&#34;]](cfg)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="iris.litmodules.get_model"><code class="name flex">
<span>def <span class="ident">get_model</span></span>(<span>cfg: Dict[str, Any], model_root: Optional[str] = None) ‑> <a title="iris.litmodules.IrisLitModule" href="#iris.litmodules.IrisLitModule">IrisLitModule</a></span>
</code></dt>
<dd>
<div class="desc"><p>Get a model, optionally by checkpoint</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>cfg: the primary model/training/data config</li>
<li>model_root: (optional) the path to a model.ckpt file</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>lit_module: a IrisLitModule object</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model(
    cfg: Dict[str, Any], model_root: Optional[str] = None
) -&gt; IrisLitModule:
    &#34;&#34;&#34;
    Get a model, optionally by checkpoint

    Arguments:
        - cfg: the primary model/training/data config
        - model_root: (optional) the path to a model.ckpt file

    Returns:
        - lit_module: a IrisLitModule object
    &#34;&#34;&#34;
    task_litmodule_map = {
        &#34;segmentation&#34;: SemanticSegmentationIrisLitModule,
        &#34;classification&#34;: MultiClassClassificationIrisLitModule,
        &#34;multilabel&#34;: MultiLabelClassificationIrisLitModule,
    }
    if model_root is None:
        # load fresh model
        return task_litmodule_map[cfg[&#34;task&#34;]](cfg)
    else:
        try:
            # load a checkpoint
            map_location = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;
            return task_litmodule_map[cfg[&#34;task&#34;]].load_from_checkpoint(
                f&#34;{model_root}/model.ckpt&#34;,
                map_location=map_location,
                cfg=cfg,
                strict=False,
            )
        except Exception as e:
            print(e)
            print(
                &#34;Checkpoint likely not found, returning requested model architecture (untrained)&#34;
            )
            return task_litmodule_map[cfg[&#34;task&#34;]](cfg)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="iris.litmodules.IrisLitModule"><code class="flex name class">
<span>class <span class="ident">IrisLitModule</span></span>
<span>(</span><span>cfg: Dict[str, Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Base Iris LightningModule</p>
<p>Aiming for compatibility/extensibility with:</p>
<p>Semantic Segmentation - DONE </p>
<p>Instance Segmentation - Use YOLO? </p>
<p>Binary Classification - TODO </p>
<p>Multi-class Classification - DONE </p>
<p>Multi-label Binary-Classification - DONE</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IrisLitModule(LightningModule):
    &#34;&#34;&#34;
    Base Iris LightningModule

    Aiming for compatibility/extensibility with:

    Semantic Segmentation - DONE \n
    Instance Segmentation - Use YOLO? \n
    Binary Classification - TODO \n
    Multi-class Classification - DONE \n
    Multi-label Binary-Classification - DONE \n
    &#34;&#34;&#34;

    def __init__(self, cfg: Dict[str, Any]) -&gt; None:
        super().__init__()

        # store hyperparameters in self.hparams
        self.save_hyperparameters(cfg)

        # initialize torch network
        self.model = self._get_torch_network()

        # loss function and loss tracking
        try:
            self.criterion = getattr(torch.nn, self.hparams.criterion)()  # type: ignore
        except Exception as e:
            self.criterion = getattr(iris_criteria, self.hparams.criterion)()  # type: ignore
        self.loss_dict = {&#34;train&#34;: [], &#34;val&#34;: [], &#34;test&#34;: []}

        # for ModelSummary Callback
        self.example_input_array = torch.randn(
            2,
            *self.hparams.imgsz,  # type: ignore
        )

        # for logging images
        self.arr2pil = T.ToPILImage()
        self.log_batches = []

    def forward(self, samples):
        return self.model(samples)

    #################### TRAINING ####################

    def training_step(
        self, batch: Tuple, batch_idx: int
    ) -&gt; Optional[Union[torch.Tensor, Dict[str, Any]]]:
        samples, targets = batch
        preds = self(samples)
        loss = self._lightning_step(&#34;train&#34;, samples, targets, preds)
        return loss

    #################### VALIDATING ####################

    def validation_step(
        self, batch: Tuple, batch_idx: int
    ) -&gt; Optional[Union[torch.Tensor, Dict[str, Any]]]:
        samples, targets = batch
        preds = self(samples)
        loss = self._lightning_step(&#34;val&#34;, samples, targets, preds)
        return loss

    #################### TESTING ####################

    def test_step(
        self, batch: Tuple, batch_idx: int
    ) -&gt; Optional[Union[torch.Tensor, Dict[str, Any]]]:
        samples, targets = batch
        preds = self(samples)
        loss = self._lightning_step(&#34;test&#34;, samples, targets, preds)
        return loss

    def on_test_epoch_end(self) -&gt; None:
        self._log_images()
        self.log_batches = []

    #################### PREDICTION ####################

    def predict_step(self, batch: Tuple, batch_idx: int) -&gt; torch.Tensor:
        samples = batch[0]
        preds = self(samples)
        # semantic segmentation models return dictionaries w/ &#34;out&#34; as the mask key
        if isinstance(preds, OrderedDict):
            return preds[&#34;out&#34;]
        else:
            return preds

    #################### OPTIMIZERS ####################

    def configure_optimizers(self) -&gt; Dict[str, Any]:
        optimizer = self.hparams.optimizer  # type: ignore
        lr = self.hparams.lr  # type: ignore
        weight_decay = self.hparams.weight_decay  # type: ignore

        if optimizer in (&#34;SGD&#34;, &#34;Adadelta&#34;):
            betas = self.hparams.momentum  # type: ignore
        if optimizer in (
            &#34;Adam&#34;,
            &#34;AdamW&#34;,
            &#34;Adamax&#34;,
        ):
            betas = (self.hparams.momentum, 0.999)  # type: ignore

        opt = getattr(torch.optim, self.hparams.optimizer)(  # type: ignore
            self.trainer.model.parameters(), lr, betas, weight_decay=weight_decay  # type: ignore
        )

        opts = {&#34;optimizer&#34;: opt}

        if self.hparams.scheduler == &#34;swa&#34;:  # type: ignore
            pass
        elif self.hparams.scheduler == &#34;linear_warmup_cosine_annealing&#34;:  # type: ignore
            #### Cosine Annealing LR with warmup (use LR finder suggestion)
            T_max = self.hparams.epochs * self.hparams.num_batches  # type: ignore
            T_0 = max(5 * self.hparams.num_batches, int(T_max / 10))  # type: ignore
            start_factor = 1e-2
            warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
                optimizer=opt, start_factor=start_factor, total_iters=T_0
            )
            annealing_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer=opt,
                T_max=T_max - T_0,
                eta_min=start_factor * self.hparams.lr,  # type: ignore
            )
            scheduler_config = {
                &#34;scheduler&#34;: torch.optim.lr_scheduler.SequentialLR(
                    opt,
                    schedulers=[warmup_scheduler, annealing_scheduler],
                    milestones=[T_0 + 1],
                ),
                &#34;interval&#34;: &#34;step&#34;,
                &#34;frequency&#34;: 1,
                &#34;monitor&#34;: &#34;val/loss_epoch&#34;,
                &#34;name&#34;: &#34;scheduler&#34;,
            }
            opts[&#34;lr_scheduler&#34;] = scheduler_config
        else:
            #### Adaptive LR reduction
            patience = max(5, int(0.05 * self.hparams.epochs))  # type: ignore
            scheduler_config = {
                &#34;scheduler&#34;: torch.optim.lr_scheduler.ReduceLROnPlateau(
                    opt,
                    mode=&#34;min&#34;,
                    factor=0.25,
                    patience=patience,
                    threshold=1e-3,
                    threshold_mode=&#34;rel&#34;,
                    cooldown=patience,
                ),
                &#34;interval&#34;: &#34;epoch&#34;,
                &#34;frequency&#34;: 1,
                &#34;monitor&#34;: &#34;val/loss_epoch&#34;,
                &#34;name&#34;: &#34;scheduler&#34;,
            }
            opts[&#34;lr_scheduler&#34;] = scheduler_config
        return opts

    #################### CUSTOM UTILITIES ####################

    def _lightning_step(
        self,
        mode: str,
        samples: torch.Tensor,
        targets: torch.Tensor,
        preds: torch.Tensor,
    ) -&gt; Optional[torch.Tensor]:
        &#34;&#34;&#34;
        General Lightning Step

        mode behavior:
            - &#34;train&#34;: Loss logging
            - &#34;val&#34;: Loss and Metric logging
            - &#34;test&#34; Loss, Metric, and Image logging
            - &#34;predict&#34; Online/Batch Inference API

        Arguments:
            - mode: the string indicating the type of lightning step to perform [&#34;train&#34;, &#34;val&#34;, &#34;test&#34;, &#34;predict&#34;]
            - samples: the batch of samples
            - targets: the batch of targets
            - preds: the batch of predictions

        Returns:
            - loss: the loss as a tensor, often singleton
        &#34;&#34;&#34;

        if mode in (&#34;train&#34;, &#34;val&#34;, &#34;test&#34;):
            loss = self._compute_loss(preds, targets)
            self.log(
                f&#34;{mode}/loss&#34;,
                loss,
                prog_bar=True,
                on_epoch=True,
                on_step=True,
                sync_dist=True,
            )

            if mode in (&#34;val&#34;, &#34;test&#34;):
                # log any attributes of self that start with &#34;metric_XXX&#34;, i.e. self.metric_mIoU
                self_attrs = dir(self)
                for attr in self_attrs:
                    if &#34;metric_&#34; in attr and attr != &#34;_metric_attributes&#34;:
                        # wandb logger errors if we call .log() with a dict
                        metric_or_dict = getattr(self, attr)(preds, targets)
                        if isinstance(metric_or_dict, dict):
                            self.log_dict(
                                {f&#34;{mode}/{k}&#34;: v for k, v in metric_or_dict.items()},
                                on_epoch=True,
                                sync_dist=True,
                            )
                        else:
                            self.log(
                                f&#34;{mode}/{attr.replace(&#39;metric_&#39;, &#39;&#39;)}&#34;,
                                metric_or_dict,
                                on_epoch=True,
                                sync_dist=True,
                            )
                if mode == &#34;test&#34; and len(self.log_batches) &lt; 5:
                    # log images
                    log_batch = {
                        &#34;samples&#34;: samples.detach().cpu(),
                        &#34;ground_truth&#34;: targets.detach().cpu(),
                    }
                    # semantic segmentation models return dictionaries w/ &#34;out&#34; as the mask key
                    if isinstance(preds, OrderedDict):
                        log_batch[&#34;predictions&#34;] = preds[&#34;out&#34;].detach().cpu()  # type: ignore
                    else:
                        log_batch[&#34;predictions&#34;] = preds.detach().cpu()
                    self.log_batches.append(log_batch)
            if loss.isnan().sum() == 0:
                return loss
            else:
                return

    def _get_torch_network(self) -&gt; torch.nn.Module:
        &#34;&#34;&#34;
        Retrieve a network from the torchvision model registry

        Returns:
            - model: a torch.nn.Module object
        &#34;&#34;&#34;
        # torchvision models
        model = torch_models.get_model(
            self.hparams.model_arch,  # type: ignore
            weights=&#34;DEFAULT&#34; if self.hparams.pretrained else None,  # type: ignore
        )

        # Determine the input space of the pytorch default final layer
        model_childrens = [name for name, _ in model.named_children()]
        try:
            final_layer_in_features = getattr(model, f&#34;{model_childrens[-1]}&#34;)[
                -1
            ].in_features
        except Exception as e:
            final_layer_in_features = getattr(
                model, f&#34;{model_childrens[-1]}&#34;
            ).in_features
        # Replace the default output layer with a custom one
        new_output_layer = torch.nn.Linear(
            in_features=final_layer_in_features, out_features=self.hparams.num_classes  # type: ignore
        )
        try:
            getattr(model, f&#34;{model_childrens[-1]}&#34;)[-1] = new_output_layer
        except Exception as e:
            setattr(model, model_childrens[-1], new_output_layer)

        return model

    def _compute_loss(
        self,
        outputs: Union[torch.Tensor, Dict[str, torch.Tensor]],
        targets: torch.Tensor,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Compute loss, defined differently for different tasks

        Arguments:
            - outputs: the outputs of the torch.nn.Module object
            - targets: the ground truth
        Returns:
            - loss: the loss tensor (often singleton)
        &#34;&#34;&#34;
        return self.criterion(outputs, targets)

    def _log_images(self, captions_batch=None, masks_batch=None) -&gt; None:
        &#34;&#34;&#34;
        Function to perform image data logging.
        &#34;&#34;&#34;
        for i, log_batch in enumerate(self.log_batches):
            samples = [self.arr2pil(sample) for sample in log_batch[&#34;samples&#34;]]
            if captions_batch is not None and masks_batch is not None:
                # both captions and masks
                self.logger.log_image(  # type: ignore
                    &#34;test/image&#34;,
                    samples,
                    caption=captions_batch[i],
                    masks=masks_batch[i],
                )
            elif captions_batch is not None:
                # captions only
                self.logger.log_image(&#34;test/image&#34;, samples, caption=captions_batch[i])  # type: ignore
            elif masks_batch is not None:
                # masks only
                self.logger.log_image(&#34;test/image&#34;, samples, masks=masks_batch[i])  # type: ignore
            else:
                # just images
                self.logger.log_image(&#34;test/image&#34;, samples)  # type: ignore</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>lightning.pytorch.core.module.LightningModule</li>
<li>lightning.fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>lightning.pytorch.core.hooks.ModelHooks</li>
<li>lightning.pytorch.core.hooks.DataHooks</li>
<li>lightning.pytorch.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="iris.litmodules.MultiClassClassificationIrisLitModule" href="#iris.litmodules.MultiClassClassificationIrisLitModule">MultiClassClassificationIrisLitModule</a></li>
<li><a title="iris.litmodules.MultiLabelClassificationIrisLitModule" href="#iris.litmodules.MultiLabelClassificationIrisLitModule">MultiLabelClassificationIrisLitModule</a></li>
<li><a title="iris.litmodules.SemanticSegmentationIrisLitModule" href="#iris.litmodules.SemanticSegmentationIrisLitModule">SemanticSegmentationIrisLitModule</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="iris.litmodules.IrisLitModule.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.
But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in
the manual optimization mode.</p>
<h2 id="return">Return</h2>
<p>Any of these 6 options.</p>
<ul>
<li><strong>Single optimizer</strong>.</li>
<li><strong>List or Tuple</strong> of optimizers.</li>
<li><strong>Two lists</strong> - The first list has multiple optimizers, and the second has multiple LR schedulers
(or multiple <code>lr_scheduler_config</code>).</li>
<li><strong>Dictionary</strong>, with an <code>"optimizer"</code> key, and (optionally) a <code>"lr_scheduler"</code>
key whose value is a single LR scheduler or <code>lr_scheduler_config</code>.</li>
<li><strong>None</strong> - Fit will run without any optimizer.</li>
</ul>
<p>The <code>lr_scheduler_config</code> is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<p>.. code-block:: python</p>
<pre><code>lr_scheduler_config = {
    # REQUIRED: The scheduler instance
    "scheduler": lr_scheduler,
    # The unit of the scheduler's step size, could also be 'step'.
    # 'epoch' updates the scheduler on epoch end whereas 'step'
    # updates it after a optimizer update.
    "interval": "epoch",
    # How many epochs/steps should pass between calls to
    # &lt;code&gt;scheduler.step()&lt;/code&gt;. 1 corresponds to updating the learning
    # rate after every epoch/step.
    "frequency": 1,
    # Metric to to monitor for schedulers like &lt;code&gt;ReduceLROnPlateau&lt;/code&gt;
    "monitor": "val_loss",
    # If set to &lt;code&gt;True&lt;/code&gt;, will enforce that the value specified 'monitor'
    # is available when the scheduler is updated, thus stopping
    # training if not found. If set to &lt;code&gt;False&lt;/code&gt;, it will only produce a warning
    "strict": True,
    # If using the &lt;code&gt;LearningRateMonitor&lt;/code&gt; callback to monitor the
    # learning rate progress, this keyword can be used to specify
    # a custom logged name
    "name": None,
}
</code></pre>
<p>When there are schedulers in which the <code>.step()</code> method is conditioned on a value, such as the
:class:<code>torch.optim.lr_scheduler.ReduceLROnPlateau</code> scheduler, Lightning requires that the
<code>lr_scheduler_config</code> contains the keyword <code>"monitor"</code> set to the metric name that the scheduler
should be conditioned on.</p>
<div class="admonition testcode">
<p class="admonition-title">Testcode</p>
<h1 id="the-reducelronplateau-scheduler-requires-a-monitor">The ReduceLROnPlateau scheduler requires a monitor</h1>
<p>def configure_optimizers(self):
optimizer = Adam(&hellip;)
return {
"optimizer": optimizer,
"lr_scheduler": {
"scheduler": ReduceLROnPlateau(optimizer, &hellip;),
"monitor": "metric_to_track",
"frequency": "indicates how often the metric is updated"
# If "monitor" references validation metrics, then "frequency" should be set to a
# multiple of "trainer.check_val_every_n_epoch".
},
}</p>
<h1 id="in-the-case-of-two-optimizers-only-one-using-the-reducelronplateau-scheduler">In the case of two optimizers, only one using the ReduceLROnPlateau scheduler</h1>
<p>def configure_optimizers(self):
optimizer1 = Adam(&hellip;)
optimizer2 = SGD(&hellip;)
scheduler1 = ReduceLROnPlateau(optimizer1, &hellip;)
scheduler2 = LambdaLR(optimizer2, &hellip;)
return (
{
"optimizer": optimizer1,
"lr_scheduler": {
"scheduler": scheduler1,
"monitor": "metric_to_track",
},
},
{"optimizer": optimizer2, "lr_scheduler": scheduler2},
)</p>
</div>
<p>Metrics can be made available to monitor by simply logging it using
<code>self.log('metric_to_track', metric_val)</code> in your :class:<code>~lightning.pytorch.core.LightningModule</code>.</p>
<h2 id="note">Note</h2>
<p>Some things to know:</p>
<ul>
<li>Lightning calls <code>.backward()</code> and <code>.step()</code> automatically in case of automatic optimization.</li>
<li>If a learning rate scheduler is specified in <code>configure_optimizers()</code> with key
<code>"interval"</code> (default "epoch") in the scheduler configuration, Lightning will call
the scheduler's <code>.step()</code> method automatically in case of automatic optimization.</li>
<li>If you use 16-bit precision (<code>precision=16</code>), Lightning will automatically handle the optimizer.</li>
<li>If you use :class:<code>torch.optim.LBFGS</code>, Lightning handles the closure function automatically for you.</li>
<li>If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them
yourself.</li>
<li>If you need to control how often the optimizer steps, override the :meth:<code>optimizer_step</code> hook.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(self) -&gt; Dict[str, Any]:
    optimizer = self.hparams.optimizer  # type: ignore
    lr = self.hparams.lr  # type: ignore
    weight_decay = self.hparams.weight_decay  # type: ignore

    if optimizer in (&#34;SGD&#34;, &#34;Adadelta&#34;):
        betas = self.hparams.momentum  # type: ignore
    if optimizer in (
        &#34;Adam&#34;,
        &#34;AdamW&#34;,
        &#34;Adamax&#34;,
    ):
        betas = (self.hparams.momentum, 0.999)  # type: ignore

    opt = getattr(torch.optim, self.hparams.optimizer)(  # type: ignore
        self.trainer.model.parameters(), lr, betas, weight_decay=weight_decay  # type: ignore
    )

    opts = {&#34;optimizer&#34;: opt}

    if self.hparams.scheduler == &#34;swa&#34;:  # type: ignore
        pass
    elif self.hparams.scheduler == &#34;linear_warmup_cosine_annealing&#34;:  # type: ignore
        #### Cosine Annealing LR with warmup (use LR finder suggestion)
        T_max = self.hparams.epochs * self.hparams.num_batches  # type: ignore
        T_0 = max(5 * self.hparams.num_batches, int(T_max / 10))  # type: ignore
        start_factor = 1e-2
        warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer=opt, start_factor=start_factor, total_iters=T_0
        )
        annealing_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer=opt,
            T_max=T_max - T_0,
            eta_min=start_factor * self.hparams.lr,  # type: ignore
        )
        scheduler_config = {
            &#34;scheduler&#34;: torch.optim.lr_scheduler.SequentialLR(
                opt,
                schedulers=[warmup_scheduler, annealing_scheduler],
                milestones=[T_0 + 1],
            ),
            &#34;interval&#34;: &#34;step&#34;,
            &#34;frequency&#34;: 1,
            &#34;monitor&#34;: &#34;val/loss_epoch&#34;,
            &#34;name&#34;: &#34;scheduler&#34;,
        }
        opts[&#34;lr_scheduler&#34;] = scheduler_config
    else:
        #### Adaptive LR reduction
        patience = max(5, int(0.05 * self.hparams.epochs))  # type: ignore
        scheduler_config = {
            &#34;scheduler&#34;: torch.optim.lr_scheduler.ReduceLROnPlateau(
                opt,
                mode=&#34;min&#34;,
                factor=0.25,
                patience=patience,
                threshold=1e-3,
                threshold_mode=&#34;rel&#34;,
                cooldown=patience,
            ),
            &#34;interval&#34;: &#34;epoch&#34;,
            &#34;frequency&#34;: 1,
            &#34;monitor&#34;: &#34;val/loss_epoch&#34;,
            &#34;name&#34;: &#34;scheduler&#34;,
        }
        opts[&#34;lr_scheduler&#34;] = scheduler_config
    return opts</code></pre>
</details>
</dd>
<dt id="iris.litmodules.IrisLitModule.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, samples) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Same as :meth:<code>torch.nn.Module.forward</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>Whatever you decide to pass into the forward method.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments are also possible.</dd>
</dl>
<h2 id="return">Return</h2>
<p>Your model's output</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, samples):
    return self.model(samples)</code></pre>
</details>
</dd>
<dt id="iris.litmodules.IrisLitModule.on_test_epoch_end"><code class="name flex">
<span>def <span class="ident">on_test_epoch_end</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Called in the test loop at the very end of the epoch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def on_test_epoch_end(self) -&gt; None:
    self._log_images()
    self.log_batches = []</code></pre>
</details>
</dd>
<dt id="iris.litmodules.IrisLitModule.predict_step"><code class="name flex">
<span>def <span class="ident">predict_step</span></span>(<span>self, batch: Tuple, batch_idx: int) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Step function called during :meth:<code>~lightning.pytorch.trainer.trainer.Trainer.predict</code>. By default, it calls
:meth:<code>~lightning.pytorch.core.LightningModule.forward</code>. Override to add any processing logic.</p>
<p>The :meth:<code>~lightning.pytorch.core.LightningModule.predict_step</code> is used
to scale inference on multi-devices.</p>
<p>To prevent an OOM error, it is possible to use :class:<code>~lightning.pytorch.callbacks.BasePredictionWriter</code>
callback to write the predictions to disk or database after each batch or on epoch end.</p>
<p>The :class:<code>~lightning.pytorch.callbacks.BasePredictionWriter</code> should be used while using a spawn
based accelerator. This happens for <code>Trainer(strategy="ddp_spawn")</code>
or training on 8 TPU cores with <code>Trainer(accelerator="tpu", devices=8)</code> as predictions won't be returned.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<p>Predicted output (optional).</p>
<p>Example ::</p>
<pre><code>class MyModel(LightningModule):

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        return self(batch)

dm = ...
model = MyModel()
trainer = Trainer(accelerator="gpu", devices=2)
predictions = trainer.predict(model, dm)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_step(self, batch: Tuple, batch_idx: int) -&gt; torch.Tensor:
    samples = batch[0]
    preds = self(samples)
    # semantic segmentation models return dictionaries w/ &#34;out&#34; as the mask key
    if isinstance(preds, OrderedDict):
        return preds[&#34;out&#34;]
    else:
        return preds</code></pre>
</details>
</dd>
<dt id="iris.litmodules.IrisLitModule.test_step"><code class="name flex">
<span>def <span class="ident">test_step</span></span>(<span>self, batch: Tuple, batch_idx: int) ‑> Union[torch.Tensor, Dict[str, Any], ForwardRef(None)]</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the test set. In this step you'd normally generate examples or
calculate anything of interest such as accuracy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch.</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># if you have one test dataloader:
def test_step(self, batch, batch_idx):
    ...


# if you have multiple test dataloaders:
def test_step(self, batch, batch_idx, dataloader_idx=0):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single test dataset
def test_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'test_loss': loss, 'test_acc': test_acc})
</code></pre>
<p>If you pass in multiple test dataloaders, :meth:<code>test_step</code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple test dataloaders
def test_step(self, batch, batch_idx, dataloader_idx=0):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to test you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>test_step</code> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_step(
    self, batch: Tuple, batch_idx: int
) -&gt; Optional[Union[torch.Tensor, Dict[str, Any]]]:
    samples, targets = batch
    preds = self(samples)
    loss = self._lightning_step(&#34;test&#34;, samples, targets, preds)
    return loss</code></pre>
</details>
</dd>
<dt id="iris.litmodules.IrisLitModule.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, batch: Tuple, batch_idx: int) ‑> Union[torch.Tensor, Dict[str, Any], ForwardRef(None)]</span>
</code></dt>
<dd>
<div class="desc"><p>Here you compute and return the training loss and some additional metrics for e.g. the progress bar or
logger.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch. This is only supported for automatic optimization.
This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</li>
</ul>
<p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example::</p>
<pre><code>def training_step(self, batch, batch_idx):
    x, y, z = batch
    out = self.encoder(x)
    loss = self.loss(out, x)
    return loss
</code></pre>
<p>To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:</p>
<p>.. code-block:: python</p>
<pre><code>def __init__(self):
    super().__init__()
    self.automatic_optimization = False


# Multiple optimizers (e.g.: GANs)
def training_step(self, batch, batch_idx):
    opt1, opt2 = self.optimizers()

    # do training_step with encoder
    ...
    opt1.step()
    # do training_step with decoder
    ...
    opt2.step()
</code></pre>
<h2 id="note">Note</h2>
<p>When <code>accumulate_grad_batches</code> &gt; 1, the loss returned here will be automatically
normalized by <code>accumulate_grad_batches</code> internally.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(
    self, batch: Tuple, batch_idx: int
) -&gt; Optional[Union[torch.Tensor, Dict[str, Any]]]:
    samples, targets = batch
    preds = self(samples)
    loss = self._lightning_step(&#34;train&#34;, samples, targets, preds)
    return loss</code></pre>
</details>
</dd>
<dt id="iris.litmodules.IrisLitModule.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, batch: Tuple, batch_idx: int) ‑> Union[torch.Tensor, Dict[str, Any], ForwardRef(None)]</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the validation set. In this step you'd might generate examples or
calculate anything of interest like accuracy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch.</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx):
    ...


# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'val_loss': loss, 'val_acc': val_acc})
</code></pre>
<p>If you pass in multiple val dataloaders, :meth:<code>validation_step</code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple validation dataloaders
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to validate you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>validation_step</code> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(
    self, batch: Tuple, batch_idx: int
) -&gt; Optional[Union[torch.Tensor, Dict[str, Any]]]:
    samples, targets = batch
    preds = self(samples)
    loss = self._lightning_step(&#34;val&#34;, samples, targets, preds)
    return loss</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="iris.litmodules.MultiClassClassificationIrisLitModule"><code class="flex name class">
<span>class <span class="ident">MultiClassClassificationIrisLitModule</span></span>
<span>(</span><span>cfg: Dict[str, Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Multi-class Classification Iris LightningModule</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiClassClassificationIrisLitModule(IrisLitModule):
    &#34;&#34;&#34;
    Multi-class Classification Iris LightningModule
    &#34;&#34;&#34;

    def __init__(self, cfg: Dict[str, Any]) -&gt; None:
        super().__init__(cfg)
        self.metric_acc = MulticlassAccuracy(self.hparams.num_classes)  # type: ignore
        self.metric_f1 = MulticlassF1Score(self.hparams.num_classes)  # type: ignore

    #################### CUSTOM UTILITIES ####################

    def _log_images(self):
        &#34;&#34;&#34;
        Log images with classification targets and predictions as image captions
        &#34;&#34;&#34;
        captions_batch = []
        for log_batch in self.log_batches:
            captions = [
                f&#34;prediction: {self.hparams.classes[torch.argmax(pred, dim=0).item()]}, \n&#34; + # type: ignore
                f&#34;target: {self.hparams.classes[target.item()]}&#34;  # type: ignore
                for pred, target in zip(
                    log_batch[&#34;predictions&#34;], log_batch[&#34;ground_truth&#34;]
                )
            ]
            captions_batch.append(captions)

        super()._log_images(captions_batch=captions_batch)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="iris.litmodules.IrisLitModule" href="#iris.litmodules.IrisLitModule">IrisLitModule</a></li>
<li>lightning.pytorch.core.module.LightningModule</li>
<li>lightning.fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>lightning.pytorch.core.hooks.ModelHooks</li>
<li>lightning.pytorch.core.hooks.DataHooks</li>
<li>lightning.pytorch.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="iris.litmodules.IrisLitModule" href="#iris.litmodules.IrisLitModule">IrisLitModule</a></b></code>:
<ul class="hlist">
<li><code><a title="iris.litmodules.IrisLitModule.configure_optimizers" href="#iris.litmodules.IrisLitModule.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.forward" href="#iris.litmodules.IrisLitModule.forward">forward</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.on_test_epoch_end" href="#iris.litmodules.IrisLitModule.on_test_epoch_end">on_test_epoch_end</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.predict_step" href="#iris.litmodules.IrisLitModule.predict_step">predict_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.test_step" href="#iris.litmodules.IrisLitModule.test_step">test_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.training_step" href="#iris.litmodules.IrisLitModule.training_step">training_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.validation_step" href="#iris.litmodules.IrisLitModule.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="iris.litmodules.MultiLabelClassificationIrisLitModule"><code class="flex name class">
<span>class <span class="ident">MultiLabelClassificationIrisLitModule</span></span>
<span>(</span><span>cfg: Dict[str, Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Multi-Label Binary Classification Iris LightningModule</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiLabelClassificationIrisLitModule(IrisLitModule):
    &#34;&#34;&#34;
    Multi-Label Binary Classification Iris LightningModule
    &#34;&#34;&#34;

    def __init__(self, cfg: Dict[str, Any]) -&gt; None:
        super().__init__(cfg)
        self._accuracy = MultilabelAccuracy(num_labels=self.hparams.num_classes, average=None)  # type: ignore
        self.metric_acc = lambda preds, targets: {
            f&#34;acc_{i}&#34;: acc_i.item()
            for i, acc_i in enumerate(self._accuracy(preds, targets))
        }
        self.metric_f1 = MultilabelF1Score(num_labels=self.hparams.num_classes)  # type: ignore

    #################### CUSTOM UTILITIES ####################

    def _log_images(self):
        &#34;&#34;&#34;
        Log images with classification targets and predictions as image captions
        &#34;&#34;&#34;
        captions_batch = []
        for log_batch in self.log_batches:
            captions = [
                f&#34;prediction: {[ self.hparams.classes[i] + &#39;(&#39; + str(round(elem, 3)) + &#39;)&#39; for i, elem in enumerate(pred.tolist()) if (elem &gt; 0.5) ]}, \n&#34; +  # type: ignore
                f&#34;target: {[ self.hparams.classes[i] for i, elem in enumerate(target.int().tolist()) if (elem &gt; 0.5) ]}&#34;  # type: ignore
                for pred, target in zip(
                    log_batch[&#34;predictions&#34;], log_batch[&#34;ground_truth&#34;]
                )
            ]
            captions_batch.append(captions)

        super()._log_images(captions_batch=captions_batch)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="iris.litmodules.IrisLitModule" href="#iris.litmodules.IrisLitModule">IrisLitModule</a></li>
<li>lightning.pytorch.core.module.LightningModule</li>
<li>lightning.fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>lightning.pytorch.core.hooks.ModelHooks</li>
<li>lightning.pytorch.core.hooks.DataHooks</li>
<li>lightning.pytorch.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="iris.litmodules.IrisLitModule" href="#iris.litmodules.IrisLitModule">IrisLitModule</a></b></code>:
<ul class="hlist">
<li><code><a title="iris.litmodules.IrisLitModule.configure_optimizers" href="#iris.litmodules.IrisLitModule.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.forward" href="#iris.litmodules.IrisLitModule.forward">forward</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.on_test_epoch_end" href="#iris.litmodules.IrisLitModule.on_test_epoch_end">on_test_epoch_end</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.predict_step" href="#iris.litmodules.IrisLitModule.predict_step">predict_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.test_step" href="#iris.litmodules.IrisLitModule.test_step">test_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.training_step" href="#iris.litmodules.IrisLitModule.training_step">training_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.validation_step" href="#iris.litmodules.IrisLitModule.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="iris.litmodules.SemanticSegmentationIrisLitModule"><code class="flex name class">
<span>class <span class="ident">SemanticSegmentationIrisLitModule</span></span>
<span>(</span><span>cfg: Dict[str, Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Semantic Segmentation Iris LightningModule (inherited from Base Iris LitModule)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SemanticSegmentationIrisLitModule(IrisLitModule):
    &#34;&#34;&#34;
    Semantic Segmentation Iris LightningModule (inherited from Base Iris LitModule)
    &#34;&#34;&#34;

    def __init__(self, cfg: Dict[str, Any]) -&gt; None:
        if &#34;ignore_index&#34; not in cfg.keys():
            cfg[&#34;ignore_index&#34;] = -100
        super().__init__(cfg)
        # loss function and loss tracking (ignoring index 0 bc it is the background)
        try:
            self.criterion = getattr(torch.nn, self.hparams.criterion)(ignore_index=self.hparams.ignore_index)  # type: ignore
        except Exception as e:
            self.criterion = getattr(iris_criteria, self.hparams.criterion)()  # type: ignore
        # Multiclass Accuracy and Multiclass Jaccard Index as performance metrics
        self._accuracy = MulticlassAccuracy(self.hparams.num_classes, ignore_index=self.hparams.ignore_index)  # type: ignore
        self._mIoU = MulticlassJaccardIndex(self.hparams.num_classes, ignore_index=self.hparams.ignore_index)  # type: ignore
        self.metric_acc = lambda preds, targets: self._accuracy(
            preds[&#34;out&#34;], targets.squeeze(1)
        )
        self.metric_mIoU = lambda preds, targets: self._mIoU(
            preds[&#34;out&#34;], targets.squeeze(1)
        )

    #################### CUSTOM UTILITIES ####################

    def _get_torch_network(self) -&gt; torch.nn.Module:
        &#34;&#34;&#34;
        Retrieve a network, either custom or torchvision

        Available Semantic Segmentation Models (self.cfg[&#34;model_arch&#34;]):
            - RITNET (custom)
            - torchvision.models.segmentation:
                - fcn_resnet50, fcn_resnet101
                - deeplabv3_mobilenet_v3_large, deeplabv3_resnet50, deeplabv3_resnet101
                - lraspp_mobilenet_v3_large

        Returns:
            - model: a torch.nn.Module object
        &#34;&#34;&#34;

        # torchvision models
        weights_backbone = &#34;DEFAULT&#34; if self.hparams.pretrained else None  # type: ignore
        try:
            model = torch_models.get_model(
                self.hparams.model_arch,  # type: ignore
                weights_backbone=weights_backbone,
                num_classes=self.hparams.num_classes,  # type: ignore
                aux_loss=True,
            )
        except Exception as e:
            try:
                model = torch_models.get_model(
                    self.hparams.model_arch,  # type: ignore
                    weights_backbone=weights_backbone,
                    num_classes=self.hparams.num_classes,  # type: ignore
                    aux_loss=False,
                )
            except Exception as e:
                print(&#34;Model Architecture not found&#34;)
                sys.exit(1)
        return model

    def _compute_loss(
        self, outputs: Dict[str, torch.Tensor], targets: torch.Tensor
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Compute loss for a segmentation network

        Arguments:
            - outputs: the outputs of the torch.nn.Module object
            - targets: the ground truth
        Returns:
            - loss: the loss tensor (often singleton)
        &#34;&#34;&#34;
        losses = {}
        for name, x in outputs.items():
            losses[name] = self.criterion(x, targets.squeeze(1))

        if len(losses) == 1:
            return losses[&#34;out&#34;]

        return losses[&#34;out&#34;] + 0.5 * losses[&#34;aux&#34;]

    def _log_images(self):
        &#34;&#34;&#34;
        Log images with predicted segmentation masks.
        &#34;&#34;&#34;
        masks_batch = []
        for log_batch in self.log_batches:
            masks = [
                {
                    &#34;predictions&#34;: {
                        &#34;mask_data&#34;: torch.argmax(pred, dim=0).numpy(),
                        # &#34;class_labels&#34;: class_labels,
                    },
                    &#34;ground_truth&#34;: {
                        &#34;mask_data&#34;: torch.squeeze(target, 0).int().numpy(),
                        # &#34;class_labels&#34;: class_labels,
                    },
                }
                for pred, target in zip(
                    log_batch[&#34;predictions&#34;], log_batch[&#34;ground_truth&#34;]
                )
            ]
            masks_batch.append(masks)

        super()._log_images(masks_batch=masks_batch)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="iris.litmodules.IrisLitModule" href="#iris.litmodules.IrisLitModule">IrisLitModule</a></li>
<li>lightning.pytorch.core.module.LightningModule</li>
<li>lightning.fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>lightning.pytorch.core.hooks.ModelHooks</li>
<li>lightning.pytorch.core.hooks.DataHooks</li>
<li>lightning.pytorch.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="iris.litmodules.IrisLitModule" href="#iris.litmodules.IrisLitModule">IrisLitModule</a></b></code>:
<ul class="hlist">
<li><code><a title="iris.litmodules.IrisLitModule.configure_optimizers" href="#iris.litmodules.IrisLitModule.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.forward" href="#iris.litmodules.IrisLitModule.forward">forward</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.on_test_epoch_end" href="#iris.litmodules.IrisLitModule.on_test_epoch_end">on_test_epoch_end</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.predict_step" href="#iris.litmodules.IrisLitModule.predict_step">predict_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.test_step" href="#iris.litmodules.IrisLitModule.test_step">test_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.training_step" href="#iris.litmodules.IrisLitModule.training_step">training_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.validation_step" href="#iris.litmodules.IrisLitModule.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="iris" href="index.html">iris</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="iris.litmodules.get_model" href="#iris.litmodules.get_model">get_model</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="iris.litmodules.IrisLitModule" href="#iris.litmodules.IrisLitModule">IrisLitModule</a></code></h4>
<ul class="">
<li><code><a title="iris.litmodules.IrisLitModule.configure_optimizers" href="#iris.litmodules.IrisLitModule.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.forward" href="#iris.litmodules.IrisLitModule.forward">forward</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.on_test_epoch_end" href="#iris.litmodules.IrisLitModule.on_test_epoch_end">on_test_epoch_end</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.predict_step" href="#iris.litmodules.IrisLitModule.predict_step">predict_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.test_step" href="#iris.litmodules.IrisLitModule.test_step">test_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.training_step" href="#iris.litmodules.IrisLitModule.training_step">training_step</a></code></li>
<li><code><a title="iris.litmodules.IrisLitModule.validation_step" href="#iris.litmodules.IrisLitModule.validation_step">validation_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="iris.litmodules.MultiClassClassificationIrisLitModule" href="#iris.litmodules.MultiClassClassificationIrisLitModule">MultiClassClassificationIrisLitModule</a></code></h4>
</li>
<li>
<h4><code><a title="iris.litmodules.MultiLabelClassificationIrisLitModule" href="#iris.litmodules.MultiLabelClassificationIrisLitModule">MultiLabelClassificationIrisLitModule</a></code></h4>
</li>
<li>
<h4><code><a title="iris.litmodules.SemanticSegmentationIrisLitModule" href="#iris.litmodules.SemanticSegmentationIrisLitModule">SemanticSegmentationIrisLitModule</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>