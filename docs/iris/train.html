<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>iris.train API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>iris.train</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#################### IMPORTS ####################
#################################################

import argparse
import os
from typing import Any, Dict, Literal, Optional, Tuple

import torch
import wandb
from lightning.pytorch import Trainer, seed_everything
from lightning.pytorch.callbacks import (
    EarlyStopping,
    LearningRateMonitor,
    ModelCheckpoint,
    ModelSummary,
    StochasticWeightAveraging,
)
from lightning.pytorch.loggers.wandb import WandbLogger
from lightning.pytorch.profilers import PyTorchProfiler
from lightning.pytorch.tuner.tuning import Tuner

from iris.data import IrisLitDataModule
from iris.litmodules import IrisLitModule, get_model
from iris.utils import load_sweep_config

#################### CUSTOM TRAINING UTILITIES ####################
###################################################################


def get_trainer(
    cfg: Dict[str, Any], debug: bool = False, n_gpus: int = 0
) -&gt; Tuple[Optional[str], Trainer]:
    &#34;&#34;&#34;
    Creates a trainer and optionally a checkpoint directory

    Arguments:
        - cfg: the primary model/training/data config
        - debug: indicator of whether or not to verbosely log
        - n_gpus: the number of gpus to train on (generally for distributed training)

    Returns:
        - checkpoint_root: if checkpointing enabled, the location to save checkpoints; else, None
        - trainer: a lightning trainer instance
    &#34;&#34;&#34;
    # callbacks
    callbacks = [
        LearningRateMonitor(),
        ModelSummary(max_depth=3),
        EarlyStopping(
            monitor=&#34;val/loss_epoch&#34;, patience=max(10, int(0.1 * cfg[&#34;epochs&#34;]))
        ),
    ]
    if cfg[&#34;scheduler&#34;] == &#34;swa&#34;:
        callbacks.append(StochasticWeightAveraging(swa_lrs=1e-2))

    # logging
    run_root = f&#34;./runs/{cfg[&#39;task&#39;]}/{cfg[&#39;dataset_name&#39;]}/{cfg[&#39;model_arch&#39;]}/&#34;
    os.makedirs(run_root, exist_ok=True)
    logger = WandbLogger(
        name=cfg[&#34;model_arch&#34;],
        project=f&#34;iris-{cfg[&#39;task&#39;]}&#34;,
        save_dir=run_root,
        anonymous=True,
        log_model=True,
    )

    # parameter/environment dependent
    local_rank = os.getenv(&#34;LOCAL_RANK&#34;)
    node_rank = os.getenv(&#34;NODE_RANK&#34;)
    checkpoint_root = None
    profiler = None
    overfit_batches = 0
    if (
        local_rank is not None
        and node_rank is not None
        and local_rank == 0
        and node_rank == 0
    ) or n_gpus &lt;= 1:
        # only save checkpoints for at most one gpu
        wandb_log_root = logger.experiment.dir[: -len(&#34;files&#34;)]
        checkpoint_root = os.path.join(wandb_log_root, &#34;weights/&#34;)
        callbacks.append(
            ModelCheckpoint(
                dirpath=checkpoint_root,
                monitor=&#34;val/loss_epoch&#34;,
                mode=&#34;min&#34;,
            )
        )
        # profiler
        if debug:
            profiler = PyTorchProfiler(
                dirpath=os.path.join(wandb_log_root, &#34;logs/&#34;), filename=&#34;perf-logs&#34;
            )
            overfit_batches = 5

    # lightning trainer configuration
    strategy = &#34;auto&#34;
    accelerator = &#34;gpu&#34;
    devices = &#34;auto&#34;
    if n_gpus &gt; 0:
        if n_gpus &gt; 1:
            devices = n_gpus
            strategy = &#34;ddp&#34;
    else:
        accelerator = &#34;cpu&#34;

    return checkpoint_root, Trainer(
        accelerator=accelerator,
        devices=devices,
        strategy=strategy,
        max_epochs=cfg[&#34;epochs&#34;],
        logger=logger,
        callbacks=callbacks,
        profiler=profiler,
        overfit_batches=overfit_batches,
        # accumulate_grad_batches=8,
        precision=cfg[&#34;precision&#34;],
    )


def tune(
    trainer: Trainer,
    lit_module: IrisLitModule,
    lit_datamodule: IrisLitDataModule,
    method: Literal[&#34;fit&#34;, &#34;validate&#34;, &#34;test&#34;, &#34;predict&#34;] = &#34;fit&#34;,
):
    &#34;&#34;&#34;
    Run batchsize finding and learning rate tuning

    Arguments:
        - trainer: a lightning trainer instance
        - lit_module: a IrisLightningModule to perform tuning
        - lit_datamodule: a IrisLightningDataModule used as the tuning dataset
    &#34;&#34;&#34;
    # lightning tuner
    tuner = Tuner(trainer)
    optimal_batch_size = tuner.scale_batch_size(
        lit_module,
        datamodule=lit_datamodule,
        method=method,
        steps_per_trial=5,
    )
    if optimal_batch_size is not None:
        lit_datamodule.hparams.batch_size = optimal_batch_size  # type: ignore
        lit_module.hparams.batch_size = optimal_batch_size  # type: ignore

    lr_finder = tuner.lr_find(
        lit_module,
        datamodule=lit_datamodule,
        method=method,
        mode=&#34;linear&#34;,
        early_stop_threshold=10,
    )
    if lr_finder is not None:
        suggested_lr = lr_finder.suggestion()
        if suggested_lr is not None:
            lit_datamodule.hparams.lr = suggested_lr  # type: ignore
            lit_module.hparams.lr = suggested_lr  # type: ignore


#################### CUSTOM TRAINING ENGINE ####################
################################################################


def train(
    data_root: str = &#34;./datasets/&#34;,
    debug: bool = False,
    run_tuning: bool = False,
    n_gpus: int = 0,
    sweep_config_fpath: Optional[str] = None,
) -&gt; None:
    &#34;&#34;&#34;
    Training Function

    1. Create a training config, dataset, and lightning modules
    2. Setup lightning callbacks, wandb logger, and profiler
    3. Tune batchsize and learning rate if necessary
    4. Train and Test the Model, with checkpointing

    Arguments:
        - cfg_fname: the filepath of the json config file
        - data_root: the root path where datasets lie
        - debug: indicator of whether or not to verbosely log
        - run_tuning: indicator of whether or not to tune the batch size and learning rate
        - n_gpus: the number of gpus to train on (generally for distributed training)
        - sweep_config_fpath: the path to a wandb sweep json config file
    &#34;&#34;&#34;

    # create config, optionally updating it from a wandb sweep config
    cfg = IrisLitDataModule.parse_config()
    if sweep_config_fpath is not None:
        cfg = load_sweep_config(cfg, sweep_config_fpath)
    seed_everything(cfg[&#34;seed&#34;])

    # get lightning datamodule
    lit_datamodule = IrisLitDataModule(cfg, root=data_root)

    # get fresh lightning module
    lit_module = get_model(cfg)

    # create and optionally tune the trainer
    checkpoint_root, trainer = get_trainer(cfg, debug=debug, n_gpus=n_gpus)
    if run_tuning and n_gpus &lt;= 1:
        tune(trainer, lit_module=lit_module, lit_datamodule=lit_datamodule)

    # run training
    trainer.fit(lit_module, datamodule=lit_datamodule)

    # checkpoint_root is only created for at most one gpu
    if checkpoint_root is not None:
        # testing best model
        trainer.test(
            lit_module,
            datamodule=lit_datamodule,
            ckpt_path=os.path.join(checkpoint_root, os.listdir(checkpoint_root)[0]),
        )

        # saving best model weights
        torch.save(
            lit_module.model,
            os.path.join(checkpoint_root, os.listdir(checkpoint_root)[0]).replace(
                &#34;.ckpt&#34;, &#34;.pth&#34;
            ),
        )


if __name__ == &#34;__main__&#34;:
    parser = argparse.ArgumentParser(
        prog=&#34;train.py&#34;,
        description=&#34;Training script for iris&#34;,
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        &#34;--debug&#34;,
        action=&#34;store_true&#34;,
        help=&#34;Run in debug mode (verbose logging, profiling, overfitted training)&#34;,
    )
    parser.add_argument(
        &#34;--tune&#34;,
        action=&#34;store_true&#34;,
        help=&#34;Tune the model for batch size, learning rate&#34;,
    )
    parser.add_argument(
        &#34;--data-root&#34;,
        default=&#34;./datasets/&#34;,
        type=str,
        help=&#34;The root where the dataset folder is located&#34;,
    )
    parser.add_argument(
        &#34;--n-gpus&#34;,
        default=torch.cuda.device_count(),
        type=int,
        help=&#34;Number of GPUs, 0 means cpu, 1 means single gpu, &gt;1 means distributed&#34;,
    )
    parser.add_argument(
        &#34;--sweep&#34;,
        help=&#34;The path to a wandb sweep config file&#34;,
        type=str,
    )
    ARGS = parser.parse_args()

    # run training
    wandb.login(key=os.getenv(&#34;WANDB_API_KEY&#34;), force=True)
    train(
        data_root=ARGS.data_root,
        debug=ARGS.debug,
        run_tuning=ARGS.tune,
        n_gpus=ARGS.n_gpus,
        sweep_config_fpath=ARGS.sweep,
    )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="iris.train.get_trainer"><code class="name flex">
<span>def <span class="ident">get_trainer</span></span>(<span>cfg: Dict[str, Any], debug: bool = False, n_gpus: int = 0) ‑> Tuple[Optional[str], lightning.pytorch.trainer.trainer.Trainer]</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a trainer and optionally a checkpoint directory</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>cfg: the primary model/training/data config</li>
<li>debug: indicator of whether or not to verbosely log</li>
<li>n_gpus: the number of gpus to train on (generally for distributed training)</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>checkpoint_root: if checkpointing enabled, the location to save checkpoints; else, None</li>
<li>trainer: a lightning trainer instance</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_trainer(
    cfg: Dict[str, Any], debug: bool = False, n_gpus: int = 0
) -&gt; Tuple[Optional[str], Trainer]:
    &#34;&#34;&#34;
    Creates a trainer and optionally a checkpoint directory

    Arguments:
        - cfg: the primary model/training/data config
        - debug: indicator of whether or not to verbosely log
        - n_gpus: the number of gpus to train on (generally for distributed training)

    Returns:
        - checkpoint_root: if checkpointing enabled, the location to save checkpoints; else, None
        - trainer: a lightning trainer instance
    &#34;&#34;&#34;
    # callbacks
    callbacks = [
        LearningRateMonitor(),
        ModelSummary(max_depth=3),
        EarlyStopping(
            monitor=&#34;val/loss_epoch&#34;, patience=max(10, int(0.1 * cfg[&#34;epochs&#34;]))
        ),
    ]
    if cfg[&#34;scheduler&#34;] == &#34;swa&#34;:
        callbacks.append(StochasticWeightAveraging(swa_lrs=1e-2))

    # logging
    run_root = f&#34;./runs/{cfg[&#39;task&#39;]}/{cfg[&#39;dataset_name&#39;]}/{cfg[&#39;model_arch&#39;]}/&#34;
    os.makedirs(run_root, exist_ok=True)
    logger = WandbLogger(
        name=cfg[&#34;model_arch&#34;],
        project=f&#34;iris-{cfg[&#39;task&#39;]}&#34;,
        save_dir=run_root,
        anonymous=True,
        log_model=True,
    )

    # parameter/environment dependent
    local_rank = os.getenv(&#34;LOCAL_RANK&#34;)
    node_rank = os.getenv(&#34;NODE_RANK&#34;)
    checkpoint_root = None
    profiler = None
    overfit_batches = 0
    if (
        local_rank is not None
        and node_rank is not None
        and local_rank == 0
        and node_rank == 0
    ) or n_gpus &lt;= 1:
        # only save checkpoints for at most one gpu
        wandb_log_root = logger.experiment.dir[: -len(&#34;files&#34;)]
        checkpoint_root = os.path.join(wandb_log_root, &#34;weights/&#34;)
        callbacks.append(
            ModelCheckpoint(
                dirpath=checkpoint_root,
                monitor=&#34;val/loss_epoch&#34;,
                mode=&#34;min&#34;,
            )
        )
        # profiler
        if debug:
            profiler = PyTorchProfiler(
                dirpath=os.path.join(wandb_log_root, &#34;logs/&#34;), filename=&#34;perf-logs&#34;
            )
            overfit_batches = 5

    # lightning trainer configuration
    strategy = &#34;auto&#34;
    accelerator = &#34;gpu&#34;
    devices = &#34;auto&#34;
    if n_gpus &gt; 0:
        if n_gpus &gt; 1:
            devices = n_gpus
            strategy = &#34;ddp&#34;
    else:
        accelerator = &#34;cpu&#34;

    return checkpoint_root, Trainer(
        accelerator=accelerator,
        devices=devices,
        strategy=strategy,
        max_epochs=cfg[&#34;epochs&#34;],
        logger=logger,
        callbacks=callbacks,
        profiler=profiler,
        overfit_batches=overfit_batches,
        # accumulate_grad_batches=8,
        precision=cfg[&#34;precision&#34;],
    )</code></pre>
</details>
</dd>
<dt id="iris.train.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>data_root: str = './datasets/', debug: bool = False, run_tuning: bool = False, n_gpus: int = 0, sweep_config_fpath: Optional[str] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Training Function</p>
<ol>
<li>Create a training config, dataset, and lightning modules</li>
<li>Setup lightning callbacks, wandb logger, and profiler</li>
<li>Tune batchsize and learning rate if necessary</li>
<li>Train and Test the Model, with checkpointing</li>
</ol>
<h2 id="arguments">Arguments</h2>
<ul>
<li>cfg_fname: the filepath of the json config file</li>
<li>data_root: the root path where datasets lie</li>
<li>debug: indicator of whether or not to verbosely log</li>
<li>run_tuning: indicator of whether or not to tune the batch size and learning rate</li>
<li>n_gpus: the number of gpus to train on (generally for distributed training)</li>
<li>sweep_config_fpath: the path to a wandb sweep json config file</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(
    data_root: str = &#34;./datasets/&#34;,
    debug: bool = False,
    run_tuning: bool = False,
    n_gpus: int = 0,
    sweep_config_fpath: Optional[str] = None,
) -&gt; None:
    &#34;&#34;&#34;
    Training Function

    1. Create a training config, dataset, and lightning modules
    2. Setup lightning callbacks, wandb logger, and profiler
    3. Tune batchsize and learning rate if necessary
    4. Train and Test the Model, with checkpointing

    Arguments:
        - cfg_fname: the filepath of the json config file
        - data_root: the root path where datasets lie
        - debug: indicator of whether or not to verbosely log
        - run_tuning: indicator of whether or not to tune the batch size and learning rate
        - n_gpus: the number of gpus to train on (generally for distributed training)
        - sweep_config_fpath: the path to a wandb sweep json config file
    &#34;&#34;&#34;

    # create config, optionally updating it from a wandb sweep config
    cfg = IrisLitDataModule.parse_config()
    if sweep_config_fpath is not None:
        cfg = load_sweep_config(cfg, sweep_config_fpath)
    seed_everything(cfg[&#34;seed&#34;])

    # get lightning datamodule
    lit_datamodule = IrisLitDataModule(cfg, root=data_root)

    # get fresh lightning module
    lit_module = get_model(cfg)

    # create and optionally tune the trainer
    checkpoint_root, trainer = get_trainer(cfg, debug=debug, n_gpus=n_gpus)
    if run_tuning and n_gpus &lt;= 1:
        tune(trainer, lit_module=lit_module, lit_datamodule=lit_datamodule)

    # run training
    trainer.fit(lit_module, datamodule=lit_datamodule)

    # checkpoint_root is only created for at most one gpu
    if checkpoint_root is not None:
        # testing best model
        trainer.test(
            lit_module,
            datamodule=lit_datamodule,
            ckpt_path=os.path.join(checkpoint_root, os.listdir(checkpoint_root)[0]),
        )

        # saving best model weights
        torch.save(
            lit_module.model,
            os.path.join(checkpoint_root, os.listdir(checkpoint_root)[0]).replace(
                &#34;.ckpt&#34;, &#34;.pth&#34;
            ),
        )</code></pre>
</details>
</dd>
<dt id="iris.train.tune"><code class="name flex">
<span>def <span class="ident">tune</span></span>(<span>trainer: lightning.pytorch.trainer.trainer.Trainer, lit_module: <a title="iris.litmodules.IrisLitModule" href="litmodules.html#iris.litmodules.IrisLitModule">IrisLitModule</a>, lit_datamodule: <a title="iris.data.IrisLitDataModule" href="data.html#iris.data.IrisLitDataModule">IrisLitDataModule</a>, method: Literal['fit', 'validate', 'test', 'predict'] = 'fit')</span>
</code></dt>
<dd>
<div class="desc"><p>Run batchsize finding and learning rate tuning</p>
<h2 id="arguments">Arguments</h2>
<ul>
<li>trainer: a lightning trainer instance</li>
<li>lit_module: a IrisLightningModule to perform tuning</li>
<li>lit_datamodule: a IrisLightningDataModule used as the tuning dataset</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tune(
    trainer: Trainer,
    lit_module: IrisLitModule,
    lit_datamodule: IrisLitDataModule,
    method: Literal[&#34;fit&#34;, &#34;validate&#34;, &#34;test&#34;, &#34;predict&#34;] = &#34;fit&#34;,
):
    &#34;&#34;&#34;
    Run batchsize finding and learning rate tuning

    Arguments:
        - trainer: a lightning trainer instance
        - lit_module: a IrisLightningModule to perform tuning
        - lit_datamodule: a IrisLightningDataModule used as the tuning dataset
    &#34;&#34;&#34;
    # lightning tuner
    tuner = Tuner(trainer)
    optimal_batch_size = tuner.scale_batch_size(
        lit_module,
        datamodule=lit_datamodule,
        method=method,
        steps_per_trial=5,
    )
    if optimal_batch_size is not None:
        lit_datamodule.hparams.batch_size = optimal_batch_size  # type: ignore
        lit_module.hparams.batch_size = optimal_batch_size  # type: ignore

    lr_finder = tuner.lr_find(
        lit_module,
        datamodule=lit_datamodule,
        method=method,
        mode=&#34;linear&#34;,
        early_stop_threshold=10,
    )
    if lr_finder is not None:
        suggested_lr = lr_finder.suggestion()
        if suggested_lr is not None:
            lit_datamodule.hparams.lr = suggested_lr  # type: ignore
            lit_module.hparams.lr = suggested_lr  # type: ignore</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="iris" href="index.html">iris</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="iris.train.get_trainer" href="#iris.train.get_trainer">get_trainer</a></code></li>
<li><code><a title="iris.train.train" href="#iris.train.train">train</a></code></li>
<li><code><a title="iris.train.tune" href="#iris.train.tune">tune</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>